{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Lab 3 : Proximal/cyclic/greedy coordinate descent\n",
    "\n",
    "#### Authors: M. Massias, P. Ablin\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- cyclic and greedy coordinate descent for ordinary least squares (OLS)\n",
    "- proximal gradient descent for sparse Logistic regression\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 11th of november at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 5 novembre 2017**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"robert\"\n",
    "ln1 = \"gower\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual functions:\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
    "    n_features = len(coefs)\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    if for_logreg:\n",
    "        b = np.sign(b)\n",
    "    return A, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ordinary Least Squares\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$.\n",
    "We want to use coordinate descent to solve:\n",
    "    $$\\hat w \\in  \\mathrm{arg \\, min \\,} \\frac 12 \\Vert Aw - b \\Vert ^2 $$\n",
    "\n",
    "We ask you to code:\n",
    "- cyclic coordinate descent: at iteration $t$, update feature $j = t \\mod p$\n",
    "- greedy coordinate descent: at iteration $t$, update feature having the largest partial gradient in magnitude, ie $j = \\mathrm{arg\\, max \\,}_{i} \\vert \\nabla_i f(w_t) \\vert$.\n",
    "\n",
    "\n",
    "**WARNING**: You must do this in a clever way, ie such that $p$ updates cost the same as one update of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "np.random.seed(1970)\n",
    "coefs = np.random.randn(n_features)\n",
    "\n",
    "A, b = simu(coefs, n_samples=1000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    residuals = b - A.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    # lips_const = \n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        j = t % n_features\n",
    "        # TODO\n",
    "        # old_w_j = \n",
    "        # w[j] += \n",
    "        # update residuals:\n",
    "        # residuals \n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append((residuals ** 2).sum() / 2.)\n",
    "    return w, np.array(all_objs)\n",
    "\n",
    "\n",
    "\n",
    "def greedy_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    gradient = A.T.dot(A.dot(w) - b)\n",
    "    gram = A.T.dot(A)  # you will need this to keep the gradient up to date\n",
    "    \n",
    "    # TODO\n",
    "    # lips_const = \n",
    "    # END TODO \n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # TODO\n",
    "        # choose feature j to update: \n",
    "        # j = \n",
    "        # old_w_j \n",
    "        # w[j] -= \n",
    "        # update gradient:\n",
    "        # gradient\n",
    "        # END TODO\n",
    "        s\n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(0.5 * np.linalg.norm(A.dot(w) - b) ** 2)\n",
    "    \n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute a precise minimum with your favorite solver\n",
    "- compare the performance of cyclic and greedy CD\n",
    "\n",
    "- could you use greedy CD for unregularized logistic regression? for OLS, but with 100,000 features? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important result\n",
    "\n",
    "Remember: we are solving \n",
    "$$\\hat w \\in \\mathrm{arg \\, min} \\sum_{i=1}^{n} \\mathrm{log} ( 1 + e^{- y_i w^\\top x_i} )  + \\lambda \\Vert w \\Vert_1$$\n",
    "1) Show that:\n",
    "$$ \\lambda \\geq \\lambda_{max} \\implies \\hat w = 0$$\n",
    "where $\\lambda_{max} := \\frac 12 \\Vert X^\\top y \\Vert_{\\infty}$.\n",
    "\n",
    "\n",
    "You will need the following beautiful result: for any $w =(w_1, \\dots, w_p) \\in \\mathbb{R}^p$, the subdifferential of the L1 norm at $w$ is:\n",
    "\n",
    "$$\\partial \\Vert \\cdot \\Vert_1 (w) = \\partial \\vert \\cdot \\vert_1 (w_1)  \\times \\dots \\times \\partial \\vert \\cdot \\vert_1 (w_p) $$\n",
    "where $\\times$ is the Cartesian product between sets,\n",
    "and $$ \\partial \\vert \\cdot \\vert_1 (w_1) = \n",
    "\\begin{cases} &w_j / |w_j| &\\mathrm{if} \\quad w_j \\neq 0, \n",
    "         \\\\ & [-1, 1] &\\mathrm{otherwise.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "(it should now be easy to find $\\partial \\Vert \\cdot \\Vert_1 (\\mathbf{0}_p)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Show that for sparse Logistic regression the coordinate-wise Lipschitz constant of the smooth term, $\\gamma_j$, can be taken equal to $\\Vert X_j \\Vert^2 / 4$, where $X_j$ denotes the $j$-th column of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to code cyclic proximal coordinate descent for sparse Logistic regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: show that is possible, when the current iterate is w, to use the better Lipschitz constant \n",
    "    $$\\sum_i  \\frac{X_{i, j}^2}{(1 + \\mathrm{e}^{-y_i X_{i, j} w_j)^2}}$$\n",
    "    \n",
    "(why is it better?)\n",
    "\n",
    "Implement it in the code with a `better_lc` parameter, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = simu(coefs, n_samples=1000, for_logreg=True)\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lamb = lambda_max / 10.  \n",
    "# much easier to parametrize lambda as a function of lambda_max than \n",
    "# to take random values like 0.1 in previous Labs\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "\n",
    "def soft_thresh(x, u):\n",
    "    \"\"\"Soft thresholding of x at level u\"\"\"\n",
    "    return np.maximum(0., np.abs(x) - u)\n",
    "\n",
    "\n",
    "def cd_logreg(X, y, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    Xw = X.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    # lips_const = \n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            # TODO\n",
    "            # grad_j = \n",
    "            # w[j] = soft_thresh(1, 2)\n",
    "            \n",
    "            # if old_w_j != w[j]:\n",
    "                # Xw += \n",
    "            #END TODO\n",
    "            \n",
    "        all_objs[t] = np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1)\n",
    "    \n",
    "    return w, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Real data\n",
    "\n",
    "We will compare vanilla cyclic CD and ISTA to solve the Lasso on a real dataset, called _leukemia_.\n",
    "\n",
    "You can download the file here: http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv, and you should place it in the same folder as the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "y = 2 * (genfromtxt('leukemia_big.csv', delimiter=',', dtype=str)[0] == 'ALL') - 1\n",
    "X = genfromtxt('leukemia_big.csv', delimiter=',')[1:].T\n",
    "\n",
    "print(X.shape)\n",
    "print(y[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max_lasso = norm(X.dot(y), ord=np.inf)\n",
    "lambd = lambda_max_lasso / 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:\n",
    "- a simple proximal gradient solver for the Lasso\n",
    "- a prox CD solver for the Lasso\n",
    "and compare them on this dataset. \n",
    "Do the plots in terms of epochs, not updates (to be fair to CD)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
