{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Lab 3 : Proximal/cyclic/greedy coordinate descent\n",
    "\n",
    "#### Authors: M. Massias, P. Ablin\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- cyclic and greedy coordinate descent for ordinary least squares (OLS)\n",
    "- proximal gradient descent for sparse Logistic regression\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 11th of november at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 5 novembre 2017**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab3_gower_robert_and_gramfort_alexandre.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"robert\"\n",
    "ln1 = \"gower\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual functions:\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
    "    n_features = len(coefs)\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    if for_logreg:\n",
    "        b = np.sign(b)\n",
    "    return A, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ordinary Least Squares\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$.\n",
    "We want to use coordinate descent to solve:\n",
    "    $$\\hat w \\in  \\mathrm{arg \\, min \\,} \\frac 12 \\Vert Aw - b \\Vert ^2 $$\n",
    "\n",
    "We ask you to code:\n",
    "- cyclic coordinate descent: at iteration $t$, update feature $j = t \\mod p$\n",
    "- greedy coordinate descent: at iteration $t$, update feature having the largest partial gradient in magnitude, ie $j = \\mathrm{arg\\, max \\,}_{i} \\vert \\nabla_i f(w_t) \\vert$.\n",
    "\n",
    "\n",
    "**WARNING**: You must do this in a clever way, ie such that $p$ updates cost the same as one update of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "np.random.seed(1970)\n",
    "coefs = np.random.randn(n_features)\n",
    "\n",
    "A, b = simu(coefs, n_samples=1000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    residuals = b - A.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis = 0) ** 2\n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        j = t % n_features\n",
    "        # TODO\n",
    "        old_w_j = w[j].copy()\n",
    "        w[j] += A[:, j].T.dot(residuals) / lips_const[j]\n",
    "        # update residuals:\n",
    "        residuals += A[:, j].dot(old_w_j - w[j])\n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append((residuals ** 2).sum() / 2.)\n",
    "    return w, np.array(all_objs)\n",
    "\n",
    "\n",
    "\n",
    "def greedy_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    gradient = A.T.dot(A.dot(w) - b)\n",
    "    gram = A.T.dot(A)  # you will need this to keep the gradient up to date\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis = 0) ** 2\n",
    "    # END TODO \n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # TODO\n",
    "        # choose feature j to update: \n",
    "        j = np.argmax(np.abs(gradient))\n",
    "        old_w_j = w[j].copy()\n",
    "        w[j] -= gradient[j] / lips_const[j]\n",
    "        # update gradient:\n",
    "        gradient += gram[j] * (w[j] - old_w_j)\n",
    "        # END TODO\n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(0.5 * np.linalg.norm(A.dot(w) - b) ** 2)\n",
    "    \n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "\n",
    "w_cyclic, f_c = cyclic_cd(A, b, n_iter)\n",
    "w_greedy, f_g = greedy_cd(A, b, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## to embed figures in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "nr_cycle = len(f_c)\n",
    "plt.plot(np.arange(nr_cycle ), f_c, label=\"cyclic_cd\")\n",
    "plt.plot(np.arange(nr_cycle ), f_g, label=\"greedy_cd\")\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compute a precise minimum with your favorite solver\n",
    "- compare the performance of cyclic and greedy CD\n",
    "\n",
    "- could you use greedy CD for unregularized logistic regression? for OLS, but with 100,000 features? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Could you use greedy CD for unregularized logistic regression? for OLS, but with 100,000 features? Explain your answers.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important result\n",
    "\n",
    "Remember: we are solving \n",
    "$$\\hat w \\in \\mathrm{arg \\, min} \\sum_{i=1}^{n} \\mathrm{log} ( 1 + e^{- y_i w^\\top x_i} )  + \\lambda \\Vert w \\Vert_1$$\n",
    "1) Show that:\n",
    "$$ \\lambda \\geq \\lambda_{max} \\implies \\hat w = 0$$\n",
    "where $\\lambda_{max} := \\frac 12 \\Vert X^\\top y \\Vert_{\\infty}$.\n",
    "\n",
    "\n",
    "You will need the following beautiful result: for any $w =(w_1, \\dots, w_p) \\in \\mathbb{R}^p$, the subdifferential of the L1 norm at $w$ is:\n",
    "\n",
    "$$\\partial \\Vert \\cdot \\Vert_1 (w) = \\partial \\vert \\cdot \\vert_1 (w_1)  \\times \\dots \\times \\partial \\vert \\cdot \\vert_1 (w_p) $$\n",
    "where $\\times$ is the Cartesian product between sets,\n",
    "and $$ \\partial \\vert \\cdot \\vert_1 (w_1) = \n",
    "\\begin{cases} &w_j / |w_j| &\\mathrm{if} \\quad w_j \\neq 0, \n",
    "         \\\\ & [-1, 1] &\\mathrm{otherwise.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "(it should now be easy to find $\\partial \\Vert \\cdot \\Vert_1 (\\mathbf{0}_p)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Show that for sparse Logistic regression the coordinate-wise Lipschitz constant of the smooth term, $\\gamma_j$, can be taken equal to $\\Vert X_j \\Vert^2 / 4$, where $X_j$ denotes the $j$-th column of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to code cyclic proximal coordinate descent for sparse Logistic regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: show that is possible, when the current iterate is w, to use the better Lipschitz constant \n",
    "    $$\\sum_i  \\frac{X_{i, j}^2}{(1 + \\mathrm{e}^{-y_i X_{i, j} w_j)^2}}$$\n",
    "    \n",
    "(why is it better?)\n",
    "\n",
    "Implement it in the code with a `better_lc` parameter, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-70e9440fd312>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-70e9440fd312>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    grad_j =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X, y = simu(coefs, n_samples=1000, for_logreg=True)\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lamb = lambda_max / 10.  \n",
    "# much easier to parametrize lambda as a function of lambda_max than \n",
    "# to take random values like 0.1 in previous Labs\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "\n",
    "def soft_thresh(x, u):\n",
    "    \"\"\"Soft thresholding of x at level u\"\"\"\n",
    "    return np.maximum(0., np.abs(x) - u)\n",
    "\n",
    "\n",
    "def cd_logreg(X, y, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    Xw = X.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis = 0) ** 2 / 4\n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            # TODO\n",
    "#             grad_j = \n",
    "            w[j] = soft_thresh(1, 2)\n",
    "            \n",
    "            # if old_w_j != w[j]:\n",
    "                # Xw += \n",
    "            #END TODO\n",
    "            \n",
    "        all_objs[t] = np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1)\n",
    "    \n",
    "    return w, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Real data\n",
    "\n",
    "We will compare vanilla cyclic CD and ISTA to solve the Lasso on a real dataset, called _leukemia_.\n",
    "\n",
    "You can download the file here: http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv, and you should place it in the same folder as the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7128)\n",
      "(72,)\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "y = 2 * (genfromtxt('leukemia_big.csv', delimiter=',', dtype=str)[0] == 'ALL') - 1\n",
    "X = genfromtxt('leukemia_big.csv', delimiter=',')[1:].T\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print (y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:\n",
    "- a simple proximal gradient solver for the Lasso\n",
    "- a prox CD solver for the Lasso\n",
    "and compare them on this dataset. \n",
    "Do the plots in terms of epochs, not updates (to be fair to CD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_lasso(x, s):\n",
    "    \"\"\"Proximal operator for the Lasso at x\"\"\"\n",
    "    return np.sign(x) * (np.maximum(abs(x)-s,0))\n",
    "    \n",
    "def lasso(x, s):\n",
    "    \"\"\"Value of the Lasso penalization at x\"\"\"\n",
    "    return s * norm(x, ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_logreg(x):\n",
    "    \"\"\"Logistic gradient\"\"\"\n",
    "    c = np.exp(- y * X.dot(x))\n",
    "    return  np.sum(- y * X.T * c / (1 + c), axis=1)\n",
    "\n",
    "# def loss_logreg(x, lbda):\n",
    "#     yXx = y * np.dot(X, x)\n",
    "#     return np.mean(np.log(1. + np.exp(- yXx))) + lbda * norm(x) ** 2 / 2.\n",
    "\n",
    "\n",
    "def loss_logreg(x):\n",
    "    yXx = y * np.dot(X, x)\n",
    "    return np.sum(np.log(1. + np.exp(- yXx)))\n",
    "\n",
    "# def lipschitz_logreg(lbda):\n",
    "#     return norm(X, ord=2) ** 2 / (4. * n_features) + lbda\n",
    "\n",
    "\n",
    "# def lipschitz_logreg(lbda):\n",
    "#     return norm(X, ord=2) ** 2 / (4.) + lbda\n",
    "\n",
    "\n",
    "def lipschitz_logreg(X):\n",
    "    return norm(X, ord=2) ** 2 / (4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def grad_logreg(x):\n",
    "#     \"\"\"Logistic gradient\"\"\"\n",
    "#     c = np.exp(- b_log * A_log.dot(x))\n",
    "#     return 1. / n_samples * np.sum(- b_log * A_log.T * c / (1 + c), axis=1)\n",
    "\n",
    "\n",
    "# def lip_logreg(A):\n",
    "#     \"\"\"Lipschitz constant for logistic loss\"\"\"\n",
    "#     return norm(A, ord = 2) ** 2. / (4. * n_samples)\n",
    "\n",
    "# step = 1 / lip_logreg(A_log)\n",
    "# s=0.05\n",
    "\n",
    "\n",
    "# def loss_logreg(x):\n",
    "#     \"\"\"Logistic loss\"\"\"\n",
    "#     return 1. / n_samples * np.sum(np.log(1. + np.exp(-b_log * A_log.dot(x))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006843439237722282\n"
     ]
    }
   ],
   "source": [
    "print (step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ista(x0, f, grad_f, g, prox_g, step, lambd, n_iter=50,\n",
    "#          verbose=True):\n",
    "#     \"\"\"Proximal gradient descent algorithm\n",
    "#     \"\"\"\n",
    "#     x = x0.copy()\n",
    "#     x_new = x0.copy()\n",
    "#     print(x)\n",
    "#     # objective history\n",
    "#     objectives = []\n",
    "#     # Current objective\n",
    "#     obj = f(x, lambd) + g(x, lambd)\n",
    "#     objectives.append(obj)\n",
    "#     if verbose:\n",
    "#         print(\"Lauching ISTA solver...\")\n",
    "#         print(' | '.join([name.center(8) for name in [\"it\", \"obj\"]]))\n",
    "#     for k in range(n_iter + 1):\n",
    "#         x = prox_g(x - step * grad_f(x), lambd)\n",
    "        \n",
    "#         obj = f(x, lambd) + g(x, lambd)\n",
    "#         objectives.append(obj)\n",
    "#         if k % 10 == 0 and verbose:\n",
    "#             print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "#                               (\"%.2e\" % obj).rjust(8)]))\n",
    "#     return x, objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ista(x0, f, grad_f, g, prox_g, step, lambd, n_iter=50,\n",
    "         verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    print(x)\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current objective\n",
    "    obj = f(x) + g(x, lambd)\n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching ISTA solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\"]]))\n",
    "    for k in range(n_iter + 1):\n",
    "        x = prox_g(x - step * grad_f(x), lambd)\n",
    "        \n",
    "        obj = f(x) + g(x, lambd)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8)]))\n",
    "    return x, objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (lambd)\n",
    "\n",
    "# def lip_logreg(A):\n",
    "#     \"\"\"Lipschitz constant for logistic loss\"\"\"\n",
    "#     return norm(A, ord = 2) ** 2. / (4. * n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.532453622789366e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lambda_max_lasso = norm(X.T.dot(y), ord=np.inf)\n",
    "lambd = lambda_max_lasso / 5.\n",
    "\n",
    "\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lambd = lambda_max / 10.  \n",
    "\n",
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "x_init = np.zeros(n_features)\n",
    "step = 1 / lipschitz_logreg(X)\n",
    "\n",
    "\n",
    "print (step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "The precise minimum for ista is:  49.90659700031606\n",
      "The precise minimizer for ista is:  [-0. -0. -0. ... -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "nr_iter = 1000\n",
    "ref_x, ref_objectives = ista(x_init, loss_logreg, grad_logreg, lasso, prox_lasso, step, lambd, nr_iter, False)\n",
    "print ('The precise minimum for ista is: ',  ref_objectives[-1])\n",
    "print ('The precise minimizer for ista is: ',  ref_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "A, b = simu_linreg(coefs)\n",
    "step = 1 / lip_linreg(A)\n",
    "ref_x, ref_objectives, ref_errors = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s, 1000, coefs, False)\n",
    "print ('The precise minimum for fista is: ',  ref_objectives[-1])\n",
    "print ('The precise minimizer for fista is: ',  ref_x)\n",
    "\n",
    "\n",
    "def ista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50,\n",
    "         x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = norm(x - x_true) / norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = f(x) + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching ISTA solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter + 1):\n",
    "\n",
    "        x = prox_g(x - step * grad_f(x), s)\n",
    "        \n",
    "        obj = f(x) + g(x, s)\n",
    "        err = norm(x - x_true) / norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
