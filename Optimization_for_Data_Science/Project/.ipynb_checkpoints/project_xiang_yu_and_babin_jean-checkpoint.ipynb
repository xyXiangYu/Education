{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT: Course Optimization for Data Science\n",
    "## Optimization strategies for the proportional odds model\n",
    "\n",
    "\n",
    "Author: Alexandre Gramfort\n",
    "\n",
    "If you have questions or if something is not clear in the text below please contact us\n",
    "by email.\n",
    "\n",
    "## Aim:\n",
    "\n",
    "- Derive mathematically and implement the loss and gradients of the proportional odds model\n",
    "- Implement your own solvers for L1 or L2 regularization with: (Accelerated) Proximal gradient descent and L-BFGS (only for L2)\n",
    "- Implement your own scikit-learn estimator for the proportional odds model and test it on the `wine quality` dataset.\n",
    "\n",
    "### Remarks:\n",
    "\n",
    "- This project involves some numerical difficulty due to the presence of many `log` and `exp` functions.\n",
    "- The correct and stable computation of the gradient is quite difficult. For this reason you have the possibility to use the `autograd` package to compute the gradient by automatic differentiation. `autograd` inspired the design of `pytorch`. It is a pure python package which makes it easy to install, and it is sufficient for our usecase.\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "This work must be done by pairs of students.\n",
    "Each student must send their work before the 20th of January at 23:59, using the moodle platform.\n",
    "This means that **each student in the pair sends the same file**\n",
    "\n",
    "On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called \"Project\".\n",
    "This is where you submit your jupyter notebook file.\n",
    "\n",
    "The name of the file must be constructed as in the next cell\n",
    "\n",
    "### Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "#### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"Yu\"\n",
    "ln1 = \"Xiang\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"project\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Proportional odds model\n",
    "\n",
    "This model is an ordinal regression model. It is a supervised learning model, in the case where the target space $Y$ is discrete: $Y=\\{1, \\dots, k\\}$; this is the case in multiclass classification for example. Its specificity is that we assume there is an order in the output space. \n",
    "\n",
    "Intuitively, it means that if the true label is 2, predicting 5 is worse that predicting 3 (as 3 is closer to 2 than 5 is). For a usual classification loss, each bad prediction costs the same. In the case of the proportional odds model, **it costs more to predict values that are farther from the true target**.\n",
    "\n",
    "The proportional odds model can be seen as an extension to the logistic regression model as we will see now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with observations in $\\mathbb{R}^p$, the proportional odds model has the following structure for $1 \\leq j \\leq k-1$:\n",
    "\n",
    "$$\n",
    "\\log \\left ( \\frac{P(Y \\leq j \\mid x)}{P(Y > j \\mid x)} \\right ) = \\alpha_j + \\beta^T x ,\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^p$ and $\\alpha = \\{ \\alpha_j \\}_{j=1}^{k-1}$ is an increasing sequence of constants ($\\alpha_1 \\leq \\alpha_2 \\leq \\dots \\leq \\alpha_{k-1}$). We omit here the last term since $P(Y \\leq k) = 1$.\n",
    "Since $P(Y > j | x) = 1 - P(Y \\leq j | x)$, we can rewrite the previous equation as:\n",
    "$$\n",
    "P(Y \\leq j \\mid x) = \\frac{e^{\\alpha_j + \\beta^T x}}{e^{\\alpha_j + \\beta^T x} + 1} = \\phi(\\alpha_j + \\beta^T x)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "P(Y = j \\mid x) = \\frac{e^{\\alpha_j + \\beta^T x}}{e^{\\alpha_j + \\beta^T x} + 1} - \\frac{e^{\\alpha_{j-1} + \\beta^T x}}{e^{\\alpha_{j-1} + \\beta^T x} + 1} = \\phi(\\alpha_j + \\beta^T x) - \\phi(\\alpha_{j-1} + \\beta^T x)\n",
    "$$\n",
    "\n",
    "for $2 \\leq j \\leq k-1$, where $\\phi$ denotes the sigmoid function $\\phi(t) = 1 / (1 + \\exp(-t))$.\n",
    "\n",
    "After one-hot encoding of the target variable ([`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), [`LabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html)), denoting $\\{ y_{ij} \\}_{j=1}^{k}$ the indicator sequence for the class of the $i^{\\text{th}}$ observation $x_i$ (i.e., exactly one of the $y_{ij}$ equals one and the rest are zero) the negative log likelihood becomes:\n",
    "\n",
    "$$\n",
    "f(\\alpha, \\beta) =\n",
    "- \\sum_{i=1}^{n} \\left [ y_{i1} \\log(\\phi(\\alpha_1 + \\beta^T x_i)) \n",
    "+ \\sum_{j=2}^{k-1} \\Big( y_{ij} \\log( \n",
    "\\phi(\\alpha_j + \\beta^T x_i) - \\phi(\\alpha_{j-1} + \\beta^T x_i)) \\Big)\n",
    "+ y_{ik} \\log(1 - \\phi(\\alpha_{k-1} + \\beta^T x_i)) \\right ] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing some $\\ell_1$ or $\\ell_2$ regularization on the parameter $\\beta$ with regularization parameter $\\lambda \\ge 0$, the penalized likelihood estimation problem reads:\n",
    "$$\n",
    "    (\\mathcal{P}_\\alpha): \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\t\\min_{\\alpha, \\beta} \\quad f(\\alpha, \\beta) + \\lambda \\mathcal{R}(\\beta) \\\\\n",
    "    \\alpha_1 \\leq \\dots \\leq \\alpha_{k-1}\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "where $\\mathcal{R}(\\beta) = \\|\\beta\\|_1$ or $\\tfrac{1}{2} \\|\\beta\\|^2_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b>\n",
    "     <ul>\n",
    "      <li>Justify that $(\\mathcal{P}_\\alpha)$ is a convex problem.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "Generate data under the above model and then estimate $\\alpha$ and $\\beta$ using maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 1000  # number of samples\n",
    "p = 2  # number of features\n",
    "k = 3  # number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate parameters and compute probability distributions for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "X = 15 * rng.normal(size=(n, p))\n",
    "alpha = np.sort(np.linspace(-10, 10, k - 1) + rng.randn(k - 1))\n",
    "beta = rng.randn(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the quantity $P(Y = j \\mid x_i)$ for $j= 1, \\dots , k$, and $i= 1, \\dots, n$.\n",
    "\n",
    "First, let us compute an array containing the values $P(Y < j \\mid x_i)$ for $j= 1, \\dots , k+1$ and $i=1, \\dots, n$. (we denote this array `F`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(t):\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "F = phi(np.dot(X, beta)[:, np.newaxis] + alpha)\n",
    "F = np.concatenate([np.zeros((n , 1)), F, np.ones((n , 1))], axis=1)\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute P(Y = j | x)\n",
    "proba = np.diff(F, axis=1)\n",
    "assert proba.shape == (n, k)\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all probas for each sample should be 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(np.sum(proba, axis=1), np.ones(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate $Y$ according to $P(Y = j \\mid x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([rng.choice(np.arange(k), size=1, p=pi)[0] for pi in proba])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(k):\n",
    "    Xj = X[y == j]\n",
    "    plt.plot(Xj[:, 0], Xj[:, 1], 'o', label='y = %d' % j, alpha=0.5)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Likelihood function\n",
    "\n",
    "We adopt the parametrization from $(\\mathcal{P}_\\alpha)$. The vector of parameters `params` has `k-1 + p` entries. The first `k-1` are the alphas $\\alpha$ and the last `p` entries correspond to $\\beta$. The function that predicts the probabilities of each sample reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_alphas(params, X=X):\n",
    "    \"\"\"Compute the probability of each sample in X.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params: array, shape (k - 1 + p,)\n",
    "        Parameters of the model. The first k - 1 entries are the alpha_j,\n",
    "        the remaining p ones are the entries of beta.\n",
    "        \n",
    "    X: array, shape (n, p)\n",
    "        Design matrix.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    proba : ndarray, shape (n, k)\n",
    "        The proba of belonging to each class for each sample.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_thresh = params.size - n_features\n",
    "    alpha = params[:n_thresh]\n",
    "    beta = params[n_thresh:]\n",
    "    F = phi(np.dot(X, beta)[:, np.newaxis] + alpha)\n",
    "    F = np.concatenate(\n",
    "        [np.zeros((n_samples , 1)), F, np.ones((n_samples , 1))], axis=1)\n",
    "    proba = np.diff(F, axis=1)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of `y` can be done with scikit-learn `LabelBinarizer`. As it's a matrix, we call it `Y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def binarize(y):\n",
    "    le = preprocessing.LabelBinarizer()\n",
    "    Y = le.fit_transform(y)\n",
    "    if Y.shape[1] == 1:\n",
    "        Y = np.concatenate([1 - Y, Y], axis=1)\n",
    "    return Y\n",
    "\n",
    "Y = binarize(y)\n",
    "Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative log-likelihood then reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negloglik_alphas(params, X=X, Y=Y):\n",
    "    proba = predict_proba_alphas(params, X)\n",
    "    assert Y.shape == proba.shape\n",
    "    return -np.sum(np.log(np.sum(proba * Y, axis=1) + np.finfo('float').eps))\n",
    "\n",
    "params = np.concatenate([alpha, beta])\n",
    "negloglik_alphas(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "      <li>Justify why applying coordinate descent or proximal gradient descent to $(\\mathcal{P}_\\alpha)$ is not easy (or even possible?).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparametrization\n",
    "\n",
    "To fix the problem, we propose to reparametrize the problem with a new vector $\\eta \\in \\mathbb{R}^{k-1}$ such that $\\alpha_j = \\sum_{l=1}^{j} \\eta_l$ with $\\eta_j \\geq 0$ for $j \\geq 2$.\n",
    "\n",
    "We denote by $\\mathcal{L}(\\eta, \\beta)$ the corresponding negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\eta, \\beta) =\n",
    "- \\sum_{i=1}^{n} \\left [ y_{i1} \\log \\left ( \\phi(\\eta_1 + \\beta^T x_i) \\right )\n",
    "+ \\sum_{j=2}^{k-1} y_{ij} \\log \\left ( \\phi(\\sum_{l=1}^j \\eta_l + \\beta^T x_i) - \\phi(\\sum_{l=1}^{j-1} \\eta_l + \\beta^T x_i) \\right ) + y_{ik} \\log \\left ( 1 - \\phi(\\sum_{l=1}^{k-1} \\eta_l + \\beta^T x_i) \\right ) \\right ] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b>\n",
    "     <ul>\n",
    "      <li>Show that $(\\mathcal{P}_\\alpha)$ can be rewritten as an unconstrained convex problem $(\\mathcal{P}_\\eta)$.\n",
    "$$\n",
    "    (\\mathcal{P}_\\eta): \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\t\\min_{\\eta \\in \\mathbb{R}^{k-1}, \\beta \\in \\mathbb{R}^{p}} \\quad \\mathcal{L}(\\eta, \\beta) + \\lambda \\mathcal{R}(\\beta) + \\sum_{j=2}^{k-1} g_j(\\eta_j)\\\\\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "          You will detail what are the functions $g_j$.\n",
    "    </li>\n",
    "    <li>\n",
    "        Justify that the problem can be solved with Proximal Gradient Descent, Proximal Coordinate Descent and the L-BFGS-B algorithm (implemented in scipy.optimize).\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b>\n",
    "     <ul>\n",
    "      <li>Introducing the functions $f_2(\\eta, \\beta) = \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2 + \\sum_{j=2}^{k-1} g_j(\\eta_j)$ (corresponding to the case where $\\mathcal{R}=\\tfrac{1}{2}\\|\\beta\\|_2^2$) and $f_1(\\eta, \\beta) = \\lambda \\|\\beta\\|_1 + \\sum_{j=2}^{k-1} g_j(\\eta_j)$ (corresponding to the case where $\\mathcal{R}=\\|\\beta\\|_1$), compute and implement the proximal operators of $f_1$ and $f_2$.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "In the code below, `lambda` being a reserved keyword in Python, we denote $\\lambda$ by `reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_f2(params, reg=1., n_classes=k):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_f1(params, reg=1., n_classes=k):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "rng = np.random.RandomState(5)\n",
    "x = rng.randn(p + k - 1)\n",
    "l_l1 = 1.\n",
    "l_l2 = 2.\n",
    "ylim = [-1, 3]\n",
    "\n",
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(x)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim(ylim)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(prox_f1(x, l_l1))\n",
    "plt.title(\"Proximal Lasso\", fontsize=16)\n",
    "plt.ylim(ylim)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(prox_f2(x, l_l2))\n",
    "plt.title(\"Proximal Ridge\", fontsize=16)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation of the solvers\n",
    "\n",
    "### L-BFGS-B Solver\n",
    "\n",
    "We will start by using the L-BFGS solver provided by `scipy`, without specifying the gradient function. In this case, the [`fmin_l_bfgs_b`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html) function will approximate the gradient using a finite difference method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 5:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the new predict_proba function using the new parametrization with $\\eta$\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(params, X=X):\n",
    "    \"\"\"Compute the probability of every sample in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : ndarray, shape (k - 1 + p,)\n",
    "        The parameters. The first k-1 values are the etas\n",
    "        and the last p ones are beta.\n",
    "        \n",
    "    X: array, shape (n, p)\n",
    "        Design matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    proba : ndarray, shape (n, k)\n",
    "        The proba of belonging to each class for each sample.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_thresh = params.size - n_features\n",
    "    eta = params[:n_thresh]\n",
    "    beta = params[n_thresh:]\n",
    "    alpha = eta.cumsum()\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return proba\n",
    "\n",
    "\n",
    "def negloglik(params, X=X, Y=Y):\n",
    "    \"\"\"Compute the negative log-likelihood.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : ndarray, shape (p + k - 1,)\n",
    "        The parameters. The first k-1 values are the etas\n",
    "        and the remaining ones are the entries of beta.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nlk : float\n",
    "        The negative log-likelihood to be minimized.\n",
    "    \"\"\"\n",
    "    proba = predict_proba(params, X=X)\n",
    "    assert Y.shape == proba.shape\n",
    "    return -np.sum(np.log(np.sum(proba * Y, axis=1) + np.finfo('float').eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation\n",
    "def alpha_to_eta(alpha):\n",
    "    eta = alpha.copy()\n",
    "    eta[1:] = np.diff(alpha)\n",
    "    return eta\n",
    "\n",
    "# Compute with P_alpha parametrization:\n",
    "negloglik_alphas(np.concatenate([alpha, beta]))\n",
    "\n",
    "# Compute with P_eta parametrization:\n",
    "eta = alpha_to_eta(alpha)\n",
    "params = np.concatenate([eta, beta])\n",
    "\n",
    "# Check that log-likelihoods match\n",
    "assert abs(negloglik(params) - negloglik_alphas(np.concatenate([alpha, beta]))) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Solve the optimization using the `fmin_l_bfgs_b` function.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "HINT: You can specify positivity contraints for certain variables using the `bounds` parameter of `fmin_l_bfgs_b`. Infinity for numpy is `np.inf`.\n",
    "\n",
    "The estimate of $\\beta$ (resp. $\\eta$ and $\\alpha$) should be called `beta_hat` (resp. `eta_hat` and `alpha_hat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = predict_proba(np.concatenate([eta_hat, beta_hat]))\n",
    "y_pred = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "for j in range(k):\n",
    "    Xj = X[y_pred == j]\n",
    "    plt.plot(Xj[:, 0], Xj[:, 1], 'o', label='y = %d' % j, alpha=0.5)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the gradients\n",
    "\n",
    "We have so far been lazy by asking `fmin_l_bfgs_b` to approximate the gradient.\n",
    "You are going to fix this using either one of the next 2 options:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 7 (option 1):</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the function grad_negloglik that computes the gradient of negloglik.\n",
    "    </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 7 (option 2):</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the function grad_negloglik that computes the gradient of negloglik\n",
    "        using the <a href=\"https://github.com/HIPS/autograd\">autograd</a> package.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**HINT** : QUESTION 7 (option 1) you can use the fact that: $\\log(\\phi(t))' = 1 - \\phi(t)$ and $\\phi(t)' = \\phi(t) (1 - \\phi(t))$\n",
    "\n",
    "You can check your implementation of the function `grad_negloglik` with the check_grad function. However **WARNING** your code is likely to be numerically quite unstable due to the numerous `log` and `exp` with tiny values that are probabilities. You may want to work with log of probabilities but **warning** this is not easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "def grad_negloglik(params, X=X, Y=Y):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def negloglik_autograd(params, X=X, Y=Y):\n",
    "    \"\"\"Compute the negative log-likelihood\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : ndarray, shape (p + k - 1,)\n",
    "        The parameters. The first k-1 values are the etas\n",
    "        and the remaining p ones correspond to beta.\n",
    "    X : ndarray, shape (n, p)\n",
    "        Design matrix.\n",
    "    Y : ndarray, shape (n, k)\n",
    "        The target after one-hot encoding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nlk : float\n",
    "        The negative log-likelihood to be minimized.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "\n",
    "grad_negloglik_auto = grad(negloglik_autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "rng = np.random.RandomState(42)\n",
    "x0 = rng.randn(p + k - 1)\n",
    "x0[1:k - 1] = np.abs(x0[1:k - 1])\n",
    "# WARNING: check_grad is likely to return a quite high value\n",
    "# due to numerical instability with exp and log with tiny\n",
    "# probability values. Don't be surprised as long as your\n",
    "# solvers below converge.\n",
    "check_grad(negloglik, grad_negloglik, x0=x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plug your gradient into L-BFGS and check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(p + k - 1)\n",
    "x0[:k - 1] = np.arange(k - 1)  # initiatlizing with etas all equal to zero is a bad idea!\n",
    "bounds = [(None, None)] + [(0, np.inf) for j in range(k-2)] + [(None, None)] * p\n",
    "x_hat, _, _ = fmin_l_bfgs_b(negloglik, fprime=grad_negloglik,\n",
    "                            x0=x0, bounds=bounds)\n",
    "Y_proba = predict_proba(x_hat)\n",
    "y_pred = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "for j in range(k):\n",
    "    Xj = X[y_pred == j]\n",
    "    plt.plot(Xj[:, 0], Xj[:, 1], 'o', label='y = %d' % j, alpha=0.5)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 9:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Wrap this into a function of X, y and lbda that implements\n",
    "        the function proportional_odds_lbfgs_l2 that will be\n",
    "        used to get a good value of x_min (minimum of the L2 regularized\n",
    "        model).\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you we give you the code of the objective to minimize\n",
    "in case you use $\\ell_1$ or $\\ell_2$ penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pobj_l1(params, X=X, Y=Y, lbda=1.):\n",
    "    n_features = X.shape[1]\n",
    "    beta = params[-n_features:]\n",
    "    n_thresh = Y.shape[1] - 1\n",
    "    eta = params[:n_thresh]\n",
    "    if np.any(eta[1:] < 0):\n",
    "        return np.inf\n",
    "    return negloglik(params, X=X, Y=Y) + lbda * np.sum(np.abs(beta))\n",
    "\n",
    "\n",
    "def pobj_l2(params, X=X, Y=Y, lbda=1.):\n",
    "    n_features = X.shape[1]\n",
    "    beta = params[-n_features:]\n",
    "    n_thresh = Y.shape[1] - 1\n",
    "    eta = params[:n_thresh]\n",
    "    if np.any(eta[1:] < 0):\n",
    "        return np.inf\n",
    "    return negloglik(params, X=X, Y=Y) + lbda / 0.5 * np.dot(beta, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportional_odds_lbfgs_l2(X, y, lbda):\n",
    "    Y = binarize(y)\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = Y.shape[1]\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return x_min\n",
    "\n",
    "x_min = proportional_odds_lbfgs_l2(X, y, lbda=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `x_min` is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = predict_proba(x_min)\n",
    "y_pred = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "for j in range(k):\n",
    "    Xj = X[y_pred == j]\n",
    "    plt.plot(Xj[:, 0], Xj[:, 1], 'o', label='y = %d' % j, alpha=0.5)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a gradient of the negative loglikelihood term we can implement other solvers. Namely you are going to implement:\n",
    "\n",
    "- Proximal Gradient Descent (PGD aka ISTA)\n",
    "- Accelerated Proximal Gradient Descent (APGD aka FISTA)\n",
    "\n",
    "Before this we are going to define the `monitor` class previously used in the second lab as well as plotting functions useful to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monitor(object):\n",
    "    def __init__(self, algo, obj, x_min, args=()):\n",
    "        self.x_min = x_min\n",
    "        self.algo = algo\n",
    "        self.obj = obj\n",
    "        self.args = args\n",
    "        if self.x_min is not None:\n",
    "            self.f_min = obj(x_min, *args)\n",
    "\n",
    "    def run(self, *algo_args, **algo_kwargs):\n",
    "        t0 = time.time()\n",
    "        _, x_list = self.algo(*algo_args, **algo_kwargs)\n",
    "        self.total_time = time.time() - t0\n",
    "        self.x_list = x_list\n",
    "        if self.x_min is not None:\n",
    "            self.err = [linalg.norm(x - self.x_min) for x in x_list]\n",
    "            self.obj = [self.obj(x, *self.args) - self.f_min for x in x_list]\n",
    "        else:\n",
    "            self.obj = [self.obj(x, *self.args) for x in x_list]\n",
    "\n",
    "\n",
    "def plot_epochs(monitors, solvers):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    for monit in monitors:\n",
    "        ax1.semilogy(monit.obj, lw=2)\n",
    "        ax1.set_title(\"Objective\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"objective\")\n",
    "\n",
    "    ax1.legend(solvers)\n",
    "\n",
    "    for monit in monitors:\n",
    "        if monit.x_min is not None:\n",
    "            ax2.semilogy(monit.err, lw=2)\n",
    "            ax2.set_title(\"Distance to optimum\")\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.set_ylabel(\"$\\|x_k - x^*\\|_2$\")\n",
    "\n",
    "    ax2.legend(solvers)\n",
    "\n",
    "\n",
    "def plot_time(monitors, solvers):\n",
    "    for monit in monitors:\n",
    "        objs = monit.obj\n",
    "        plt.semilogy(np.linspace(0, monit.total_time, len(objs)), objs, lw=2)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Timing\")\n",
    "        plt.ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    plt.legend(solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 8a:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the proximal gradient descent (PGD) method\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "The parameter `step` is the size of the gradient step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd(x_init, grad, prox, n_iter=100, step=1., store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Proximal gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 8b:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Using the monitor class and the plot_epochs function, display the convergence.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "NOTE: You will have to provide a `step` value, which should be theoretially less than `1 / lipschitz_constant`. You will propose a value for it but you are not expected to provide a mathematical proof, unless you think it's a moral duty to give one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you we give you the proximal operator functions for $\\ell_1$ and $\\ell_2$ regularized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_l1(params, step, lbda, n_classes):\n",
    "    return prox_f1(params, reg=step * lbda, n_classes=n_classes)\n",
    "\n",
    "def prox_l2(params, step, lbda, n_classes):\n",
    "    return prox_f2(params, reg=step * lbda, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = np.zeros(p + k - 1)\n",
    "x_init[:k - 1] = np.arange(k - 1)\n",
    "n_iter = 10000\n",
    "lbda = .1\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the $\\ell_1$ regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbda = 1.\n",
    "\n",
    "# Run PGD for L1\n",
    "monitor_pgd_l1 = monitor(pgd, pobj_l1, x_min=None, args=(X, Y, lbda))\n",
    "monitor_pgd_l1.run(x_init, grad_negloglik, prox_l1, n_iter, step,\n",
    "                   grad_args=(X, Y), prox_args=(lbda, k))\n",
    "\n",
    "monitors = [monitor_pgd_l1]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 9:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the accelerated proximal gradient descent (APGD) and add this solver to the monitoring plots.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apgd(x_init, grad, prox, n_iter=100, step=1., store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Accelerated proximal gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    y = x_init.copy()\n",
    "    t = 1.\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbda = 0.1\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2, monitor_apgd_l2]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbda = 1.\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l1, monitor_apgd_l1]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Application\n",
    "\n",
    "You will now apply your solver to the `wine quality` dataset. Given 11 features\n",
    "that describe certain wines (our samples), the objective it to predict the quality of the wine,\n",
    "encoded by integers between 3 and 8. Rather than using a multiclass classification\n",
    "model we're going to use a proportional odds model.\n",
    "\n",
    "Let's first inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv', delimiter=';')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1] - 3\n",
    "X.shape, y.shape, np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a basic scaling of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the functions above with this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = np.zeros(X.shape[1] + np.unique(y).size - 1)\n",
    "Y = binarize(y)\n",
    "negloglik(x_init, X=X, Y=Y)\n",
    "grad_negloglik(x_init, X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate our experiment we're going to write a full scikit-learn estimator.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 10:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the `fit` method from the estimator in the next cell\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class ProportionalOdds(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"scikit-learn estimator for the proportional odds model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : float\n",
    "        The regularization parameter\n",
    "    penalty : 'l1' | 'l2'\n",
    "        The type of regularization to use.\n",
    "    max_iter : int\n",
    "        The number of iterations / epochs to do on the data.\n",
    "    solver : 'pgd' | 'apgd' | 'lbfgs'\n",
    "        The type of regularization to use.\n",
    "        'lbfgs' is only supported with penalty='l2'.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha_ : ndarray, (n_classes - 1,)\n",
    "        The alphas.\n",
    "    beta_ : ndarray, (n_features,)\n",
    "        The regression coefficients.\n",
    "    \"\"\"\n",
    "    def __init__(self, lbda=1., penalty='l2', max_iter=2000,\n",
    "                 solver='lbfgs'):\n",
    "        self.lbda = lbda\n",
    "        self.penalty = penalty\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "        assert self.penalty in ['l1', 'l2']\n",
    "        assert self.solver in ['pgd', 'apgd', 'lbfgs'] \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target. Must be integers between 0 and n_classes - 1.\n",
    "        \"\"\"\n",
    "        n_classes = int(np.max(y)) + 1\n",
    "        assert np.all(np.unique(y) == np.arange(n_classes))\n",
    "        Y = binarize(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO\n",
    "\n",
    "        # END TODO\n",
    "        self.params_ = x\n",
    "        self.alpha_ = eta.cumsum()\n",
    "        self.beta_ = beta\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            The predicted target.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict proba method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_proba : ndarray, shape (n_samples, n_classes)\n",
    "            The predicted probabilities.\n",
    "        \"\"\"\n",
    "        return predict_proba(self.params_, X)\n",
    "\n",
    "\n",
    "for solver in ['pgd', 'apgd', 'lbfgs']:\n",
    "    clf = ProportionalOdds(lbda=1., penalty='l2', max_iter=1000, solver=solver)\n",
    "    clf.fit(X, y)\n",
    "    print('Solver with L2: %s   -   Score : %s' % (solver, clf.score(X, y)))\n",
    "\n",
    "for solver in ['pgd', 'apgd']:\n",
    "    clf = ProportionalOdds(lbda=1., penalty='l1', max_iter=1000, solver=solver)\n",
    "    clf.fit(X, y)\n",
    "    print('Solver with L1: %s   -   Score : %s' % (solver, clf.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 11:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Compare the cross-validation performance of your model with a multinomial\n",
    "        logistic regression model that ignores the order between the classes. You will comment your results.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
