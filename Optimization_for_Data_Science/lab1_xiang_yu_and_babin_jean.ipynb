{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 : First order methods on regression models\n",
    "\n",
    "#### Authors: A. Gramfort, R. Gower, P. Ablin\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- proximal gradient descent (ISTA)\n",
    "- accelerated gradient descent (FISTA) \n",
    "\n",
    "for \n",
    "- linear regression\n",
    "- logistic regression \n",
    "\n",
    "models.\n",
    "\n",
    "The proximal operators we will use are the \n",
    "- ridge penalization\n",
    "- L1 penalization\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 7th of october at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 2 octobre 2017**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab1_xiang_yu_and_babin_jean.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"Yu\"\n",
    "ln1 = \"Xiang\"\n",
    "fn2 = \"Jean\"\n",
    "ln2 = \"Babin\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to embed figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 : Introduction\n",
    "\n",
    "We'll start by generating sparse vectors and simulating data\n",
    "\n",
    "### Getting sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "norm = np.linalg.norm\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Parameters / Coefficients')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features = 50\n",
    "n_samples = 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = ((-1) ** idx) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the simulation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a linear regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape (n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape (n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape (n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # multivariate_normal:  Draw random samples from a multivariate normal distribution\n",
    "    \n",
    "    # Simulation of the labels\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    return A, b\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "def simu_logreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a logistic regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape (n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape (n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape (n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(A.dot(coefs))\n",
    "    b = np.random.binomial(1, p, size=n_samples)\n",
    "    b = 2 * b - 1\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Proximal operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind that the proximal operator of a function $g$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{prox}_g(x) = \\arg\\min_z \\left\\{ \\frac{1}{2} \\Vert x - z\\Vert_2^2 + g(z) \\right\\}.\n",
    "$$\n",
    "\n",
    "\n",
    "We have in mind to use the following cases\n",
    "\n",
    "- Ridge penalization, where $g(z) = \\frac{s}{2} \\|z\\|_2^2$\n",
    "- Lasso penalization, where $g(z) = s \\|z|\\|_1$\n",
    "\n",
    "where $s \\geq 0$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Questions</b>:\n",
    "     <ul>\n",
    "      <li>Code a function that computes $g(x)$ in both cases and $\\text{prox}_g(x)$ for ridge and  lasso penalization (use the slides of the first course to get the formulas), using the prototypes given below</li>\n",
    "      <li>Visualize the functions applied element wise by the proximity operators of the Ridge and Lasso \n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_lasso(x, s):\n",
    "    \"\"\"Proximal operator for the Lasso at x\"\"\"  \n",
    "    \"\"\"s is the regularation parameter\"\"\"\n",
    "    # x is the observation\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - s, 0)   \n",
    "    \n",
    "def lasso(x, s):\n",
    "    \"\"\"Value of the Lasso penalization at x\"\"\"    \n",
    "    return s * norm(x, ord=1)\n",
    "\n",
    "def prox_ridge(x, s):\n",
    "    \"\"\"Proximal operator for the ridge at x\"\"\"   \n",
    "    return x / (1. + s)\n",
    "    \n",
    "def ridge(x, s):\n",
    "    \"\"\"Value of the ridge penalization at x\"\"\"\n",
    "    return s / 2. * norm(x) ** 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We are now going to visualize the effect of the proximity operators on coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAEKCAYAAABe0sceAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xu8XGV97/Hvj50ENqBuIlHJhghVGkqLGsnh0vQoUjGIHoiILdRqsXJie0prbU800ddR9NgSm17UqlWqFDlSRC3EVNAoF7WlcgmGO0QRMGSDcgkBAptcdn7nj2dNmExmZq89s27Pms/79ZrX3rPmmbWemVnzm+e2nsfcXQAAAACAeO1RdgYAAAAAAP2hYgcAAAAAkaNiBwAAAACRo2IHAAAAAJGjYgcAAAAAkaNiBwAAAACRo2KXMzN7g5l928weM7NnzewnZvYJM9tvCvs4zszczI7r4fjnmFmua1qY2f1mdkGex4iVmY0kn8Gry84LkBUzOzOJSY3bU2Z2i5mdbWbTSshPKTEozXHN7ODkPTqroGwBA4vYlP64TbGpcZsws1+Y2UVmdlCHtGdmcWzkp/CTfJCY2Qcl/ZWklZLOkrRR0pGSPiDprWb2Ond/IMWufizpWEl39pCNL0r6Tg/PQzZGJH1E0gaFzxGok7cpnNvPT/7/R0kvkvThgvPxFklPFnxMANVFbErvXEmrJM2QdIxCmeXXzOxod9+WpHlIoRz6s3KyiLSo2OXEzF4n6eOSPunu72t66AdmdpmkmyRdKOl1XfYxJMnc/UlJ1/WSD3ffoBDcas/Mpkva7u659lCWzcxM0nR331p2XjDwbnb3e5L/v2tmL5f0XnUoPOV17rr72iz3ByB6xKb07nX3Rhnzh0lZ6uMKHRHXSZK7b1GP5VAUi6GY+Xm/Qg/dstYH3P0+ScslHWdmRze2J93cf2VmS83sPklbJR3RbiimmQ2Z2cfN7CEze8bMrjazw5J05zSl220oZpLm42b2Z2Z2XzJU4Qdm9ust6d5gZlc0HeN2M/vLpMI5JU2v4a1mdoGZPW5mTyZd/i9sSXu2mf3IzDaa2SYzu87M3tSSpjEs4H+Z2d+Y2YOStkgaMbNZZvaFZNjrM2b2gJn9q5mNtuzjnGQfh5nZajN72szWm9m7ksffYWZ3m9lmM7vGzF7W5nUtToZ5PGtmj5rZl8xsZiOPku5Lkv5z03CHM5uef2ry+p5JXuvXzWxOyzHuN7OvmNkfmtndCufFLu8HUBE3Snq+mb1I6n7umtkBZnZh8r3ZYma3mtnvN3ZkZnuY2feTfbygafsRZjZuZiuatu0y9MeeG471m2b2tSTG/dLMliWPn2hma5Pv/I1mdmTzi8gy9qUxhZj1q2Z2mZk9nMSc9UnMmJY8vq+Z/WOyfUuS7kozO6xpH883s8+Y2YNJmnVm9j4zszxeG1ARxKb0GqOLdpZFrMNQTDN7b/IanzWzNWb239vt0Mxen7yuZ83sHjM7y0JZ8P6WdHtbuFzpPjPbmvz9kJlRX0mJHrscJD+yr5X0TXd/tkOyVZI+Iel4Sdc3bT9T0r2S/rekpyU9KOkFrU+W9FFJH5S0QtKVCi0rq6aQzd+XtE6hBWtGsp9vmtlh7r49SfMrkq5SGMLwrKT5ks6RNEvS0ikcq9knk/yeIelQSX8tabZ27bk8WGEI6f0K5+j/kPQtM3uju7cOK/2QQsBeLGkoyeec5O8ySY8k+/9LSdcmr6/1M/m6pH+W9LeS/pek883sUEnHJa9zuqRPSfpXSc0V8eXJfj8taYmkUYVWrt8ws99UGLpwqqRL9dxQBykZymBmfyTpnyT9i6SPSXqewvv7AzN7hbs/1ZTH10l6lcLn/nDy3gBVc4ikCUmbm7btdu6a2T6SfiBpP4U49oBCTPp/Zra3u5/n7juSwtQtkr4g6XQzG5b0VUl3KHz3J/NlhZER5ykMx/prMxuRdJLCMPnNkv5G0koze1lTa30esa+bmUoXsy6X9LikP5b0qELMOUnPNdL+g6STFd7Tn0p6oaQFCkPClRSOLpf0aoWei9sUCrN/n7y2D+bw2oAqIDald3Dyt+uwSzN7t0KZ7gJJl0h6uaSLFcoyzekOV4g7N0g6XaHM+X8UyrY7mtJNk7Ra0uGS/q9CfDomSTtTISZiMu7OLeObpBdLcknndkmzV5Lmc03bXKEiN9yS9rjkseOS+/spfOk/15LuL5J05zRtOyd8zLukc4Uf/elN205Ltv9mh/yaQiXrQwoFiz2aHrtf0gWTvCeN1/Cdlu1vT7b/dofn7ZEc97sKFeXG9oOT5/1YYbhqt2MPSTooSf+W1vdG0jubtu0nabukxyQ9v2n7nyVpX9p0/AlJH2451oIk3aKWfJ7Vkm5fSU9IOr9l+yEKLYd/3vL+PiPpJWWf29y4ubsUGqBc0tzk+7mfpPck34mVTenanruSzm6OaU3br1QoYA01bXtLkvZdCoWgpyQd2vK8XWJQU/4+3LRtWrLvbZIOadp+cpL2tR1ea7+xr20MmOQ5u8UsSfsn90/u8rzbJf19l8ffnOzjzJbtX1QY8bB/2ecWN2793IhN7Y/b4fmN2LQ42c/eCp0NGyR9o0PaM5P7eyhUelvLdL+bpGt+zf+q0GC1d9O2AxQqpvc3bXtH8tzXtOzzQwrloheVfX7FcKNrs3q+4+7jk6Q5QtI+Cj1Nzb4xheN8z5+7KFYKLSPSrl3vByTDg36u8KXaptAjNaJwEXIvvtZy/+sKLTbHNh33SDP7lpn9UqGStU3SCQqButVKT775zczsjy0Mkdyc7GN98lC7fXy78Y+7P64QYK/zcG1jw93J38ZMUScoBLaLzGxa46bQ+/qUpNe0OU6zYxUu6m59/gPJsVqff527/2KSfQJFu1vh+7lR0uckXSTpD1vStDt3XyNpzN2/37L9Kwotz4c3Nrj7ZQqt4v8k6X9K+jN3/2nK/DV/t7dLukfSTzwMh29+DdJz3+28Yl9XKWLWYwqjOZab2f9MRhW0ulHSmWb2QTOb32Z41msU4u2/tmz/ikIr+rEC6oHYlN4Xkv08rdAb+EuFXspuDkxurWW6f1OIX82OkXSFuz/T2ODuD0n6r5Z0J0r6uaT/aikXfVdh5NQxqV/RAKNil4/HFFoiDu6SpvFY66yYD6XY/wHJ34dbtv8yxXMbNrbc35L83UvaOWRnlUIL78cVWnH+m8IQgZ3perBLHj0ML3hcYUiRLEyxe5VCt/ufSvrN5Ljf6XDM3d4vM/tThUB+pcJQyKP0XEBot4/HW+5v7bCt+fmNAHqPQkBsvj1PYQhUN43nX9nm+Ue0eX6a8wIo2lsUvp+HSdrH3d/p7q2xpd25O7PD9l80Pd7sy5L2VIh5rZWSbqb83c4x9nWUJmYlDVgnSFqjMLT7J2Z2r5n9cdOu/lShkPaHCpW8h83sH8xs7+TxmZI2+u4TRHR634FYEZvS+3iyn9dK+ozCUO3PTfKcRjm0tUzXGPHUmra1vLrbcxXKRS/V7mWiG5LHJytXQVxjlwt3325mP5B0gpnt5e2vszs5+Xt169NTHKIRdF6kMJ674cVTy2lXL1MYu/0Od/9KY6OZ/Y8+97tLHs1shsJQibFk04kK465/x8OMno10e6u9du/X6ZKucved47HN7JB+Mt1GI3C9QbsH4+bHJ3v+mdr1M2x4quV+rWf6RLRu9+dmnuuk3bm7Ue17z1/S9Liknd/98xWGGR6qMPHU+3Z/ambyin3dpIpZ7n6vpHeamUl6pcKwsc+Z2f3u/m1336xwnd4yM3upwhD75QoFxA8ovK8zzWxGS+Vut/cdiByxKb2fu/ua5P8fmtnzJL3LzD7v7jd0eE6jHNpappum9g3T7XoTW8usjylMOPc7HY55f4ftaEKPXX7+VuHk/uvWB5If7A9I+qG7X9/6eAq3KXSZv61le+v9fjQqUjuHa1qYAvftfe639Qv7NoXz8EddjvurCteupbV38/MT75rC89P4nsKQpjnuvqbNrTGcotETOtzy/P9SqLy9vMPz12WcX6BKfiDpQDNr/V7/nkLLbvOanZ9S6NE/RWG24fea2cIc85ZX7JvsmKljlgc3K1xXLUm/0SbNz9397xR+LxqP/0Ah3rb+VrxdofL3IwGDjdgUJmEZV1jPrpMNCiPOWst0b9XunUbXSTqpuYHezA7Q7uW67ygMO93coVz0aA+vZeDQY5cTd7/SzD4i6aMWpr2/UKFn59UKX5onFC4U7WXfj5vZJyV90MyeUhi+82pJ706S7Oj45PTuUhjr/FdmNqEQSLJoifp1M/sXhdmjflVhCMH33f2q5PErFcZnX2hmf6fQhf9RhetN0jZEfEfSBywsEH+DwnCF0zLI+07u/jMz+4Skz5jZXIUfg2cVgtIJkr7o7tcoDDV4TGHWrFsVKuT3uftjZrZE0mfNbJbCePsnFH4kXqvwnkxlWAcQkwsUZuS91Mw+pFBIeLvCd+c97j4hSWb2VklnKbRQ3yvp02b2BklfTmaObTe8p195xb4jzWxTm+2rlCJmmdkrFAqSlygMAR9S6PHfrmTkh5n9KNnfbQoTbL1WoWfvy8luvi3pPyV9Pok7dyjMwneWwmRfFJww6C7Q4MWmXbj7L8zss5L+t5kd6e43tUmzw8w+KumLTWW6lyuUb1sXZP+4QjxbbWZ/qzB09f8olI+ay6sXKTRoXZWU/25RuPb3ZQqj3BY1X6eH9qjY5cjdP2ZmNyh88f5FobVlvUIl79w2472n4iMKMyK9W2HGxusVfuSvVagg9MXdt5rZIoXx1hcqDD84XyH//9zHrt+r8AW9RKFg8u8K+W8c9w4ze7vC9P+rFKbbXaowRPO4lMf4mMKFxO9TGHP+A0kLFSYeyIy7f9DM7pL0J8nNFVqwrlKYdbQR/M5S6Lm9UuE79y6FGaO+YGYPKCyV8HvJY2OS/kPSzVnmFagSd3/azF6rMJ33coXrUtepaYhRcr3tP0u6qHnYkcL351ZJF5jZm9pNntRn3vKKfX+U3FrNUrqY9YskD3+hMGnBswoVuDc3Fbx+qNCCvlQhntwr6X3u/unkte2wsCboXyuMGnmhwvCmv1CYthwYaAMam9r5hMJsoh9W6JFsl58vmdm+CvHjDIUhqWcoTDTTnO7OJO6sUJhsZSzZ/4lqmovC3bclPZ5LFWbqPEShMfxnCsslZLp4fF1ZxucdSmRmpynMMvkad/+PsvPTzMLi6tdIOsHdryw5OwAAAChBUiG8R9Ll7v7uydIjPXrsImVmRyssLHu9QsvtkQqtHNcpDLUBAAAASmVm/6gwt8CDkmYrjN7aT2F4OTLU9+QpZnaQmV1jZnea2R1m9t42aczMPm1m95jZrWb26n6PC21WWG/lQoXrM96r0MV9Utbd/0CsiE8AqojYhAGzl8Lwy+8qLOb+tKTXu/utpeaqhvoeipnMbHOAu/84mSL1JoULHO9sSnOSwvo6J0k6WtKn3P3ovg4MAJMgPgGoImITgDz03WPn7g+5+4+T/59SmLVntCXZKZIuTKZovk7SSBLUACA3xCcAVURsApCHTK+xS6b1n6dw3VezUYXZAhs2JNseakknM1usMBuO9tlnnyMPO+ywLLMIoGQ33XTTo+4+q+jj9hufiE1AvRGbAFRV2viUWcUumeHm3yT9ubu3rmGRmrufpzD+VvPnz/c1a9ZklEMAVWBmPy/hmH3HJ2ITUG/EJgBVlTY+9T0UMznYdIXAdJG7X9omyZjCws0NBybbACBXxCcAVURsApC1LGbFNElfknSXu/99h2SrJL0zmeHpGElPuPtuwzABIEvEJwBVRGwCkIcshmIukPQOSbeZ2c3Jtg9KmiNJ7v55SVcozOp0j6RnJL0rg+MCwGSITwCqiNgEIHN9V+zc/T8l2SRpXNKf9HssAJgK4hOAKiI2AchDJtfYAQAAAADKQ8UOAAAAACJHxQ4AAAAAIkfFDgAAAAAiR8UOAAAAACJHxQ4AAAAAIkfFDgAAAAAil8UC5cjAyrVjWrF6nR7cNK7ZI8NasnCuFs0bLTtbAAAAACJAxa4CVq4d07JLb9P4tglJ0timcS279DZJonIHAAAAYFIMxayAFavX7azUNYxvm9CK1etKyhEAAACAmFCxq4AHN41PaTsAAAAANKNiVwGzR4antB0AAAAAmlGxq4AlC+dqePrQLtuGpw9pycK5JeUIdbJy7ZgWLL9ahyy9XAuWX62Va8fKzhIAAAAyxuQpFdCYIOX937hVWyd2aJRZMZERJuYBAAAYDFTsKmLRvFFdfMN6SdIl7zm25NygLrpNzEPFDgAAoD4YignUGBPzAAAADAYqdkCNMTEPAADAYKBiB9QYE/MAAAAMhkwqdmZ2vpk9bGa3d3j8ODN7wsxuTm4fzuK4ALpbNG9U5556hGYMha/66Miwzj31iIG5vo7YBKCqiE8AspbV5CkXSPqMpAu7pPkPd39zRscDkNKAT8xzgYhNAKrpAhGfAGQokx47d/+hpI1Z7AsAskJsAlBVxCcAWSvyGrtjzewWM/u2mf16gccFgG6ITQCqivgEILWi1rH7saSXuvtmMztJ0kpJh7ZLaGaLJS2WpDlz5hSUPQADitgEoKpSxSdiE4CGQnrs3P1Jd9+c/H+FpOlmtn+HtOe5+3x3nz9r1qwisgdgQBGbAFRV2vhEbALQUEjFzsxeYmaW/H9UctzHijg2AHRCbAJQVcQnAFOVyVBMM7tY0nGS9jezDZI+Imm6JLn75yWdJumPzWy7pHFJp7u7Z3FsAOiE2ASgqohPALKWScXO3c+Y5PHPKEzpO5BWrh3TitXr9OCmcc0eGdaShXMHZh0xoEzEJgBVRXwCkLWiJk8ZWCvXjmnZpbdpfNuEJGls07iWXXqbJFG5AwAAAJCJIpc7GEgrVq/bWalrGN82oRWr15WUIwAAAAB1Q8UuZw9uGp/SdgAAAACYKip2OZs9Mjyl7QAAAAAwVVTscrZk4VwNTx/aZdvw9CEtWTi3pBwBAAAAqBsmT8lZY4KU93/jVm2d2KFRZsWMAjOZAgAAICZU7AqwaN6oLr5hvSTpkvccW3JuMBlmMgUAAEBsGIoJtGAmUwAAAMSGih3QgplMAQAAEBsqdkALZjIFAABAbKjYAS2YyRQAAACxYfIUoAUzmQIAUH3MYA3siood0AYzmQIAUF3MYA3sjqGYAAAAiAozWAO7o8cOAAAAUWEGa+Qp1mG+VOzQUawnNQAAqLfZI8Maa1OJYwZr9CvmYb4MxURbjZN6bNO4XM+d1CvXjpWdNQAAMOCYwRp5iXmYLxU7tBXzSQ0AAOpt0bxRnXvqEZoxFIqyoyPDOvfUIyrfo4Lqi3mYL0Mx0VbMJzUAAKg/ZrBGHmIe5ptJj52ZnW9mD5vZ7R0eNzP7tJndY2a3mtmrszgu8tPp5I3hpAYaiE0Aqor4BFRTzMN8sxqKeYGkE7s8/kZJhya3xZL+KaPjIicxn9RAkwtEbAJQTReI+ARUTszDfDMZiunuPzSzg7skOUXShe7ukq4zsxEzO8DdH8ri+Mhe4+R9/zdu1daJHRplVkxEiNgEoKqIT0B1xTrMt6hr7EYlPdB0f0OybbfgZGaLFVqmNGfOnEIyh/ZiPamBKSA2AaiqVPGJ2ASgoXKzYrr7ee4+393nz5o1q+zsAIAkYhOAaqprbFq5dkwLll+tQ5ZergXLr2a5JSCFonrsxiQd1HT/wGQbAJSJ2ASgqgY2PsW8QDRQpqJ67FZJemcyw9Mxkp5gjDiACiA2AaiqgY1PrKUL9CaTHjszu1jScZL2N7MNkj4iabokufvnJV0h6SRJ90h6RtK7sjguAHRDbAJQVcSnzlhLF+hNVrNinjHJ4y7pT7I4FgCkRWwCUFXEp85iXiAaKFPlJk8BAADA4GIt3Tgx4U35ipo8BQAAAJgUa+nGhwlvqoGKHQAAACqFtXTj0m3CGyp2xWEoJgAAAICeMeFNNVCxAwAAANCzThPbMOFNsQZiKObKtWNasXqdHtw0rtmM00aNcG4DAICyLVk4d5dr7CQmvClD7St2XMyJuuLcRhFoPACAdAY5XjLhTTXUfihmt4s5gZhxbiNvjcaDsU3jcj3XeMAU1gCwK+JlqNzNmzOiow+ZqWuXHk+lrgS1r9hxMSfqinMbeaPxAADSIV6iCmpfseNiTtQV5zbyRuMBAKRDvEQV1L5it2ThXA1PH9plGxdzog44t5E3Gg8AIB3iJaqg9hW7RfNGde6pR2jGUHipoyPDOvfUIxj3i+hxbiNvNB4gTyvXjmnB8qt1yNLLtWD51QN1LRLqh3iJKqj9rJhSKABffMN6SdIl7zm25NwA2eHcRp6Y5Qx5YVZf1A3xElUwEBU7AEBvaDxAHrpNNEFBGLEiXqJstR+KCQAAqoWJJgAge/TYJQZ5UUkAAIo0e2RYY20qcUw0AQC9o8dOLCoJAECRmGgCALJHj50Y619V9KJWC58HgKww0QQAZC+THjszO9HM1pnZPWa2tM3jZ5rZI2Z2c3I7K4vjZoWx/tVDL2q1xPx5xB6fgLpaNG9U8+aM6OhDZurapccPXKWO2AQUr+7LrPTdY2dmQ5I+K+kESRsk3Whmq9z9zpakl7j72f0eLw+M9S9Wmp4felGrJdbPow7xCUD9xBCbGKWBuhmEZVay6LE7StI97n6vu2+V9FVJp2Sw38Iw1r84aXt+eu1FrXtLTFki7tWOPj4BqKVKx6aYR2kAnXRrpK6LLCp2o5IeaLq/IdnW6q1mdquZfcPMDuq0MzNbbGZrzGzNI488kkH2Jrdo3qjOPfUIzRgKb8foyLDOPfWI2tTeqyTtl6pTb2m3XlR+iPLTy+dREZnFpzJiE4DaqnRsGoQCMAZPxI3UqRU1K+a/SzrY3V8h6XuSvtwpobuf5+7z3X3+rFmzCsoeY/2LkvZL1UsvKj9E+al5r3aq+FRWbAIwsEqLTYNQAMbgibiROrUsKnZjkppbkQ5Mtu3k7o+5+5bk7hclHZnBcRGhtF+qXnpR+SHKT8S92sQnAFVU6dg0CAVgDJ6aN1JLyqZid6OkQ83sEDObIel0SauaE5jZAU13T5Z0VwbHRYSm8qWaai9qGT9Eg3RNX6S92sQnAFVU6dg0CAVgDJ6IG6lT63tWTHffbmZnS1otaUjS+e5+h5l9TNIad18l6c/M7GRJ2yVtlHRmv8dFnPJcu2jJwrm7zHYk5ftD1G52pSVfv0Uf/fc7tOmZbcwiVgHEp+6Y9Q4oR9VjU9rfamIIYrNo3qguvmG9JOmS9xxbcm6yl8kC5e5+haQrWrZ9uOn/ZZKWZXEsxC+vL1XRC962u6Zv2w7X489sk1TPaXRjRHxqbxCmfQaqrOqxabLfamJI9VDRrp6iP5OiJk9BBgZp2F+vihwumObaPSZvQVUx2RCAfhBDqoWZwaunjM+Eil0k+MJWT9pr95i8BVXEZEMA+kEMKdZkjfv9VLTpOMhHGY0fVOwiUfeWsRiDSruLy9thFjFUEbPeAegHMaQ4aRr3e61o03GQnzIaP6jYRaLOLWOxBpXW2ZVGhqdr+pDtkoZZxFBVzHoHoB/EkOKkadzvtaJd946DMpXR+EHFLhJ1bhmLOag0X9N380feoBWnvbLUaXRj7PlEOQZh2ue64nuOKiCGFCdN436vFe2iOw4GKX6V0fiRyayYyF/RU/kXqVtQOXC/uCquZU6j222GMqCduk/7XEfMRIgqIYYUY/bIsMbalJWaG/d7nRk8zb6z0il+rfn5Rl1z9yO1m82z6NnaJXrsolHnlrE690YWKeaeTwDp8D0HpqYOPURpe356mRm8yF6lTvHrouvWR3c5TlpFztYu0WMXlbq2jHXrjWy83kHTy7onder5BNBena+3BrJWlx7uTj0/krRg+dW7lBWy2nce70+nOOUt9xuNVTF9RlVBjx1KV+feyF70OpkMPZ9A/fE9B9KrUw93a8+PpLZlhUc3b+l733mVv6YSp2is6g0VO/QlqyEORXdVV1mvP0TMUAbUH99zIL0693B3Kis8sLG6r61d/LIOaYtsrKrDcN0GKnboWazLFFRdrz9E9HwC9cf3HEivzj3cncoEWyd2FJyT9NrFr7cfM6fUxqq6lWW5xi5yvVyLlZVuPUsUMnrXzwxVdb0OE8Bz+J4D6dR5RvFOZYVGpamq2sWv+S+dmck1fr2UibuVZWOcn6Danz66KruVoc5DHMrEUCsAAPpX1R7uLIb+dSorHDQzvspIFpfj9FomrltZNvqKXZ3GxU5V2RcF13mIQ5mq+kMEAEBsqnYNf1aN8p3KCvvvu2cOua6+XsvEdSvLRl2xK7vHqmxltzLQs5Sfqv0QAVUyyA16APpXZgzJslGessJzei0T160sG3XFruweq7L12srQa0BrfZ4kepYAFGrQG/SALAxy40jZMaTsRvm66rVMXLdRUlFPnjLoX45eLgrudbHOTs8799QjNG/OiCQu4geQPyZtAvpTl0W7e9VrDOl1srrW543sPV2PP7Ntt3SxDv2rin4myqnThFRR99jVbVzsVPXSytBrL+eg944CqIZBb9AD+jXov+e9xJBee/naPW/zs9s1fWjX1dtiHvpXFXXreetVJj12ZnaipE9JGpL0RXdf3vL4npIulHSkpMck/a6739/vces8jW1aU21l6LVQ1O15MU4Hi8FRVnxK07rbLo2kXba97rBZuubuRyZtJS5y6ZMs8z3Zvlv306m1+wXD07Vg+dVdj58mj3mlyXLfvZxH/eR7suP1+vrb/Vb3su9ez7WylRWbBr1xpJclhXrt5Wv3vG07XCPD0/XM1om+p/fHrurU89arvit2ZjYk6bOSTpC0QdKNZrbK3e9sSvZuSY+7+8vN7HRJn5D0u/0eu/ElyGLti0HR6xpp/aytBpSlrPiUZqhTuzRLvn6LZNK2Cd+57SvXrd+5305DpoocWpVlvtPsu3U/0/cwTR+ynceSpOl7mJ7eul2bxrd1fF6aPOaZJqt9N97Hbu9ZlsdPc972+vqXXXqbZo/stXMWv1733cu5VrYyy06D/nveS6dA1o3iT4xv01GHzJQ0uBUQ5MPcffJU3XZgdqykc9x9YXJ/mSS5+7lNaVYnaX5kZtMk/UIb+qmZAAAanklEQVTSLJ/k4PPnz/c1a9ZMmod/OeNP9ZJHHtDhBzxfknT/Y09Lkg5+4T4709z50JOSNKU07dK1e16aNFkdv12aNPtubNt3z2m699GntWPHc2/9HnuYfmX/fbR5y/aOeXrR8/bs+LyHn9qS+vhTeR1TfW3d9t3L55hmP+3SVfH4vbyPUzlHN80+RKd88W+Vhpnd5O7zUyXuU17xabLYtGD51RrbNK733PpN/coTzw3V2XPakPbbZ7ok6fGnt2nL9olOu+iqeT8Hv3AfrV2/qeu+9pwW1jbaf989ezrPmtNMdqyp5Lv1WGn3PW1oD03scLm79pw2pAl3bZ/Y0VOeYrTntCHtOT0MN9qybUfPn0cvx836vTYzPW+vaX2fW60a5xqxaXeNCvQ7b7p0Z3zaY4/wOQxPH+q77JLm96XdtqzKTmnSPLp5i372yNM7Y8hBM4fbloEa++oUr9vFtDTxsvk7nHXZqdcySJFll7RlwLKPn1fZaarlJil9fMpiKOaopAea7m+QdHSnNO6+3cyekPRCSY+27szMFktaLElz5sxJlYGZ++ypvZ94bqrSZ7bu/iXae8auU5mmSdMuXbvnpUmT1fHbpUmz78a2xol236NPa2KH71Lga1TQ2u270aLa7nmNYJjm+FN5HVN9bd3208vnmGY/7dJV8fi9vI9TOUc3Pr37uVMRmcWnqcSmTq20W7ZP6Jmte+z8v1fN+0mzry3bJ3Tvo+HHppfzrDlNlvluPVbafW+f2KGXvGAvSSGmXXfvYz3nKUZbtk/sLEw+Ob77sNQ8j5s1d995fmW5/8a5RmzaXaMn86c3XybpuYafh5/aMml86LV8U2TZKU2a5rJLo1zUrgzU2NdBM4fbNm433rdOx+r2vMnKTo9u3qJfPrlF7q7Hn96mg2YOp/p977UM0svz0uax1zJgFmWnXt/HdtuyPI/zik1Z9NidJulEdz8ruf8OSUe7+9lNaW5P0mxI7v8sSbNbxa5Z2h67Vr/7hR9J6t69nSZNu3TtnpcmTVbHTyNNHrM6Vpr9rFw7ltlw2axeW9HvUdnHT7OfXs7jXvJUcKt4LvEpbY9dq9GR4Z3XpG54fLxtmjSa93PJe47teLzJntdJt8807bHSHr/5WFN5HdcuPT6TPMWo+fXH/trzei1pz/VmgxCbusnqNyjt71seZad+yhuT5bHTdcxp8pTm+ufWY7UbLtrrZCBZ/b73mseyyk55v49p85hmP5NJG5+ymBVzTNJBTfcPTLa1TZMMJ3iBwoXAqLnGl2prMnSnqmtOrVw7prXrN+n6+zZWdk2fGPJYQaXEpzQLnrZL07h+rJt214K021c7WUyOkGW+0+w7zX7SPC9NHvNMk9W+ez2Psjp+GmmPn+a19JLHSCZRo+yUsbzLG4vmjerapcfrvuVvmtJi4L08L4aZS8ljNWVRsbtR0qFmdoiZzZB0uqRVLWlWSfqD5P/TJF092fV1qEdBPoYvVQyVzxjyWFGlxKfGtMujI8MytZ92uV2aFW97pVac9spdtv3+MXO67qfdvoasfcE3i8kRssx3mn338vrbPS9NHvNMk9W+ez2Pejl+p/NoZHh6Jq8/zWvJ4j2qKMpOGYuhvJFWDDOXksdq6vsau2Tc99mSVitM2Xu+u99hZh+TtMbdV0n6kqT/Z2b3SNqoEMDQRaeCvFTtmb5axfClimHB4xjyWEVlxqdF80ZTVWTapenlM23eV6fhJ0sWzt05FXQ/ssx32n1n9byy02S179b0aY81leN3Oo/OOfnXc3v9WX6OVUbZKXsxlDfSimHm0rzz2Ojc2DqxQwuWX91TL3wM72PWMlmg3N2vcPdfdfeXuftfJds+nAQmufuz7v42d3+5ux/l7vdmcdw6q0vLUwyLyMfwYxBDHqtqEONTmh5DYDKcR/kaxNiUpxjKG2mlGc5ftjzz2Klz49HNU5twJIb3MWuZLFCO7NWlIB/DIvIxtOjEkEdMrl0LZF6F5F57voBmnEfoV1FxL4byRlqN9yfNpCtlyTOPnTo3Htg4vnOW9rLzWFVU7CqqLgX5GL5Uef8YZDGcoE4/WIOqLsOrASCtbnEvazGUN6YihkaVvPLYqRNjaw9raMbwPmaJil1F1akgX/UvVZ4/Bp1+1GaP7EWr04DhOkmgOEX2jqOzbnGvsRxFlqpe3kA6nTo3ZgxlcgVZbqoQd6jYVRQF+WLl9WOQ1XACiR+s2NVleDVQdfSOV0e3uJdHxQ710KlzY/bIXqXlqdPoq8a2V330u3p663ZtmwgT15YVd6jYTUHRNXEK8vHLcjgB4laX4dVA1dE7Xh3EvfJlcTlI0Tp1bmQxq3Mv2jUWLfn6LZJpZ0Vu0/i23Z5XRtypdp9mhbCOGHrR6cer6sMJkL1BnJ0LKAO949VB3CtXVrNLlqF1YXdJpa3t3K6xaNsO31mp66bouEPpMqW6LD+AYnX6UTtoJq2Vg4ap44Fi1Gna+9gR98rV7XKQmJTdudJP5azouMNQzJQYJ45eVG04AcrF8Gogf3WafKwOiHvlqcvlIEVPwtOq05DiyZQRd6jYpcQ4cfSq3Y9a1St2VZjZCQB6weRjQBDr7JKtyu5caddYNH0P2+Uau8a2ffeapk3PbCst7lCxS6lbC2DVC+lAs1hmdkI5qNSjDuglAqoxu2QWvylld650aixqt63suEPFLqVuLYBU7BCLmGZ2QvGYJh4A6qPsy0Gy+k2pQudKp8aiqv02UrGbAloAy1WXnoQyX0enmZ3SYEa5+st7mvgYp90GgJj1ejlIFvE6q98UOlfSo2KHKHRr9YlJ2a8jppmdULw8p4nvdO7PHtlL+++7Z9/7BwBkI6t4neVvCp0r6cR19SQGVl2Wmyj7dfRaOWNGucGQ5zTxdZl2GxhUjR6cMtYRQ7GyitcsPVI8KnaIQl0WnC37dbRbV2/6HqbpQ7bbtv32ns66QwMmz8WE6zLtNjCIyl5HDMXKKl6zQH3xGIqJKJQ9I1JWyn4dMc3shOLlOU18XabdBgZR3tffolqyitcsPVI8KnaIQhVmRMpCFV5HLDM7oRx5XcdQhWm3AfSm7NEmKFaW8Zpr44pFUymisGjeqM499QiNjgyXNjwwi+sLqvA6gDJ0OveZOAWoPq6VylfVrl9sF6/feuSoHtg4Xpk8or2+euzMbKakSyQdLOl+Sb/j7o+3STchqTH133p3P7mf42Iwldnqk+X6XrReFYP4VD29TrsN1EmMsanbaBP0p6rrhzbH66rmEbvrt8duqaSr3P1QSVcl99sZd/dXJTcKTYhO2bNZoifEJwBVFF1sYrRJfmIoX8SQRwT9XmN3iqTjkv+/LOn7kj7Q5z6ByuH6gigRnwBUUZSxidEm+YihfBFDHhH022P3Ynd/KPn/F5Je3CHdXma2xsyuM7NF3XZoZouTtGseeeSRPrMHZDN2nesLopRpfCI2AcgIsQk7xVC+iCGPCCat2JnZlWZ2e5vbKc3p3N0leYfdvNTd50v6PUmfNLOXdTqeu5/n7vPdff6sWbOm8lqA3XQaF/7o5i1T2g9rsVRTkfGJ2AQgLWIT0oqhfBFDHhFMOhTT3V/f6TEz+6WZHeDuD5nZAZIe7rCPseTvvWb2fUnzJP2stywD6XUaF/7AxvEpzcbHWizVRHyamkbv9daJHVqw/GrOYSAnxCakFUP5IoY8Iuj3GrtVkv5A0vLk7zdbE5jZfpKecfctZra/pAWS/qbP4wKpdBr/3ejBmwquL4gO8akJs5oBlUFswi5iKF/EkEf0f43dckknmNlPJb0+uS8zm29mX0zS/JqkNWZ2i6RrJC139zv7PG5fqrZeCPLTafz3jCGWcBwAUcanvDCrGVAZxCYAueirx87dH5P02222r5F0VvL/f0k6op/jZIlW68HSae2d2SN7lZgrFCHG+JQnZjUDqoHYhDIxJL/eBq7bglbrwdJp7Z2pXF8H1AGzmgFAtmIbAdapc6Pq+UZ6/V5jFx1arQdPu3HhF9+wvqTcAOXo1HvNrGYAMHUxjgDr1rlR1Txjagaux45Wa8TWwgZkoVPvNT/mAIpWh9/hGEeA1a1zow7nUdYGrseOVut8VX3sdowtbEBWmNUMQNk6/Q7PHtkrqsskYqwkzR4Z1lib/MXYuUF5rr2B67Gj1To/MYzdjrGFDQCAuui2vmxMYhwBVqeFxinPtTdwPXYSrdZ5iWHsdowtbAAA1EWW68uWKcYRYHVaaJzyXHsDWbFDPmL4ktVpGAIAAFko8jKKTr/Dsa0vG2slqS6dG5Tn2ovrW4RKi2FYQp2GIQAA0K+iL6Po9Dt80MzqlBXSWjRvVNcuPV73LX+Trl16fC0qTLGgPNceFTtkJoYvGddYAgDwnKKvVWJ9WWQh1vJc3jN5MhQTmYllWEJdhiEAANCvMi6jYH1ZZCG28lwRM3lSsUOmYvuSAQAwyLhWCShGEZMMMhQTAABgQMVwGQVQB0X0jlOxAwAAGFCxXKuU97VJQN6KmGSQoZgAAAADrOqXURRxbRKQtyLWPqTHDgAAAJWV98yd9AaiCEX0jtNjBwAAgMrK89qkbr2BQNby7h2nxw5Ro5UNg4jzHsAgyfPapKLX8QPyRMUO0erUykYhF3XGeQ9g0OQ5c2cZ6/gBeemrYmdmbzOzO8xsh5nN75LuRDNbZ2b3mNnSfo4JNNDKhm7qGp8474G41TU25SnPa5OKmKkQKEq/PXa3SzpV0g87JTCzIUmflfRGSYdLOsPMDu/zuMhYjEO7aGXDJGoZnzjvgejVMjblbdG8UV279Hjdt/xNunbp8Zldp8Q6fqiTvip27n6Xu0/WTHyUpHvc/V533yrpq5JO6ee4yFasQ7toZUM3dY1PnPdA3Ooam2IVyzp+QBpFXGM3KumBpvsbkm1tmdliM1tjZmseeeSR3DPXrxh7ulrFOrSrTq1sdTiPIpU6PlUlNtXpvAfQUXSxKWZ59QYCRZu0YmdmV5rZ7W1uubQcuft57j7f3efPmjUrj0NkplNP16Obt5Scs6mJdWhXXVrZYu0xrYIi41NVYlNdznugzgYxNvWDxk0gG5OuY+fur+/zGGOSDmq6f2CyLXqderoe2Diu/ffds6RcTd3skWGNtanExTC0K+/1QIrQrcc09teWt0GNT3U474E6G9TY1Itu68gR54CpKWIo5o2SDjWzQ8xshqTTJa0q4Li569Sj1QhOsWBoV7li7TGtidrGJwBRG5jYFOvlIEAV9bvcwVvMbIOkYyVdbmark+2zzewKSXL37ZLOlrRa0l2Svubud/SX7Wro1KM1Yyiu5QEZ2lUuJsPIx6DHJwDVRGzaFY2bQHYmHYrZjbtfJumyNtsflHRS0/0rJF3Rz7GqaMnCuVp26W27tDQNTx/S7JG9SsxVbxjaVZ5O5xE9pv0Z9PgEoJqITbuK+XIQoGri6lpKocgLcDv1dMV0fR3KR48pAGBQcTkIkJ2+euyqpowLcNv1dF18w/pcjoX6oscUADCIGr99K1av04ObxjV7ZFhLFs7lNxHoQa0qdswuCAAAEBcaN4Fs1GooJhfgAsVi7SEAAIBqqFXFjtkFgeKwsDoAAEB11KpixwW4QHFYewgAAKA6anWNHRfgAsVh6DMAAEB11KpiJ3EBLlAU1h4CAACojloNxQRQHIY+AwCaMaEWUC4qdiUg8KEOWFgdANDAhFpA+Wo3FLPqylhEHcgLQ58BABJrCQNVQI9dwfKeSZDeQAAAUDQm1ALKR49dwfIMfN16AwEAAPLChFpA+eixK1iei6izrhgAACgDE2oB5aNiV7A8Ax/DIAAAQBmYUAsoH0MxC5bnIuoMgwAAAGVhQi2gXFTsSpBX4FuycK6WXXrbLsMxG72BF9+wPvPjAQAAAKgGhmLWCMMgAAAAgMHUV4+dmb1N0jmSfk3SUe6+pkO6+yU9JWlC0nZ3n9/PcdEZwyCAgPgEoIqITQDy0u9QzNslnSrpCynSvs7dH+3zeACQFvEJQBURmwDkoq+KnbvfJUlmlk1uACAjxCcAVURsApCXoq6xc0nfNbObzGxxQccEgDSITwCqiNgEYEom7bEzsyslvaTNQx9y92+mPM5vufuYmb1I0vfM7G53/2GH4y2WtFiS5syZk3L3AAZRkfGJ2AQgLWITgDJMWrFz99f3exB3H0v+Pmxml0k6SlLbip27nyfpPEmaP3++93tsAPVVZHwiNgFIi9gEoAy5D8U0s33M7HmN/yW9QeHC4VpauXZMa9dv0vX3bdSC5Vdr5dqxsrMEoINBi09VQ7wE2iM2AehFXxU7M3uLmW2QdKyky81sdbJ9tpldkSR7saT/NLNbJN0g6XJ3/04/x62qlWvHtOzS27R1YockaWzTuJZdehuFFaAExKdqI15iUBGbAOSl31kxL5N0WZvtD0o6Kfn/Xkmv7Oc4sVixep3Gt03ssm1824RWrF7H2nJAwYhP1Ua8xKAiNgHIS1GzYg6EBzeNT2k7AAwq4iUAANmiYpeh2SPDU9oOAIOKeAkAQLao2GVoycK5Gp4+tMu24elDWrJwbkk5AoBqIl4CAJCtvq6xw64a14WsWL1OD24a1+yRYS1ZOJfrRQCgBfESAIBsUbHL2KJ5oxRMACAF4iWAqmosx7J1YocWLL+ahidEgaGYAAAAQILlWBArKnYAAABAottyLECVUbEDAAAAEizHglhRsQMAAAASLMeCWFGxAwAAABIsx4JYMSsmAAAAkGA5FsSKih0AAADQhOVYECOGYgIAAABA5KjYAQAAAEDkqNgBAAAAQOSo2AEAAABA5KjYdbFy7ZjWrt+k6+/bqAXLr9bKtWNlZwkAAAAAdkPFroOVa8e07NLbtHVihyRpbNO4ll16G5U7AAAAAJVDxa6DFavXaXzbxC7bxrdNaMXqdSXlCAAAAADa66tiZ2YrzOxuM7vVzC4zs5EO6U40s3Vmdo+ZLe3nmEV5cNP4lLYDqJY6xycA8SI2AchLvz1235P0G+7+Ckk/kbSsNYGZDUn6rKQ3Sjpc0hlmdnifx83d7JHhKW0HUDm1jU8AokZsApCLvip27v5dd9+e3L1O0oFtkh0l6R53v9fdt0r6qqRT+jluEZYsnKvh6UO7bBuePqQlC+eWlCMAU1Hn+AQgXsQmAHmZluG+/lDSJW22j0p6oOn+BklHd9qJmS2WtDi5u9nM0l7Utr+kR1OmTWWP4efPHNp35qgNTZvhE9u3TmzeOPaWjz+5McNDZJ7nyXztjzLZTeH5zkiM+Y4xz1L3fL+0yIwk+o5PVYpNBSHfxYkxz1L98j1osUmK8zOMMc8S+S5SjHmWMig7TVqxM7MrJb2kzUMfcvdvJmk+JGm7pIvSHLQbdz9P0nlTfZ6ZrXH3+f0ev0gx5lki30WKMc9ScfkuMj4NUmySyHeRYsyzRL4nOUblY1OSh+g+wxjzLJHvIsWYZymbfE9asXP310+SiTMlvVnSb7u7t0kyJumgpvsHJtsAoC/EJwBVRGwCUIZ+Z8U8UdL7JZ3s7s90SHajpEPN7BAzmyHpdEmr+jkuAEyG+ASgiohNAPLS76yYn5H0PEnfM7ObzezzkmRms83sCklKLhA+W9JqSXdJ+pq739HncdvpaRhCyWLMs0S+ixRjnqVq5Lsq8akK70UvyHdxYsyzRL57VZXYJJX/XvQixjxL5LtIMeZZyiDf1n4EAAAAAAAgFv322AEAAAAASkbFDgAAAAAiF33FzsxONLN1ZnaPmS0tOz+dmNn5Zvawmd3etG2mmX3PzH6a/N2vzDy2Y2YHmdk1Znanmd1hZu9Ntlc272a2l5ndYGa3JHn+aLL9EDO7PjlXLkkuSK8cMxsys7Vm9q3kfuXzbWb3m9ltyfUia5JtlT1HikJ8yk+MsUmKOz4Rm+qD2JQfYlPxiE3PibpiZ2ZDkj4r6Y2SDpd0hpkdXm6uOrpA0okt25ZKusrdD5V0VXK/arZL+kt3P1zSMZL+JHmPq5z3LZKOd/dXSnqVpBPN7BhJn5D0D+7+ckmPS3p3iXns5r0KF8s3xJLv17n7q5rWYKnyOZI74lPuYoxNUtzxidhUA8Sm3BGbikdsanD3aG+SjpW0uun+MknLys5Xl/weLOn2pvvrJB2Q/H+ApHVl5zHFa/impBNiybukvSX9WNLRkh6VNK3duVOVm8JaRVdJOl7StyRZJPm+X9L+LduiOEdyfE+IT8XmP6rYlOQvmvhEbKrPjdhUeP6JTfnmldjUdIu6x07SqKQHmu5vSLbF4sXu/lDy/y8kvbjMzEzGzA6WNE/S9ap43pNu+ZslPSzpe5J+JmmThymkpeqeK59UWN9oR3L/hYoj3y7pu2Z2k5ktTrZV+hwpAPGpIDHFJina+ERsqg9iU0GITYUgNjWZllXu0B93dzOr7NoTZravpH+T9Ofu/qSZ7Xysinl39wlJrzKzEUmXSTqs5CxNyszeLOlhd7/JzI4rOz9T9FvuPmZmL1JYm+nu5gereI4gvSp/frHFJim++ERsQlVV+fMjNuWP2LS72HvsxiQd1HT/wGRbLH5pZgdIUvL34ZLz05aZTVcIThe5+6XJ5ijy7u6bJF2j0BU/YmaNxowqnisLJJ1sZvdL+qrCsIJPqfr5lruPJX8fVvgxOEqRnCM5Ij7lLObYJEUVn4hN9UJsyhmxqTDEphaxV+xulHRoMvvNDEmnS1pVcp6mYpWkP0j+/wOFcdiVYqGJ6UuS7nL3v296qLJ5N7NZSWuTzGxYYWz7XQpB6rQkWaXyLEnuvszdD3T3gxXO5avd/e2qeL7NbB8ze17jf0lvkHS7KnyOFIT4lKMYY5MUZ3wiNtUOsSlHxKbiEJvaKPviwX5vkk6S9BOFccAfKjs/XfJ5saSHJG1TGO/7boVxwFdJ+qmkKyXNLDufbfL9WwrjgG+VdHNyO6nKeZf0CklrkzzfLunDyfZfkXSDpHskfV3SnmXntctrOE7St2LId5K/W5LbHY3vYZXPkQLfG+JTfnmOLjYl+Y46PhGb6nEjNuWaZ2JTOfknNrnLkh0BAAAAACIV+1BMAAAAABh4VOwAAAAAIHJU7AAAAAAgclTsAAAAACByVOwAAAAAIHJU7AAAAAAgclTsAAAAACBy/x9CDqnvGpGTOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = randn(50) # 50 random numbers from the “standard normal” distribution\n",
    "l_l1 = 1.     # regularation parameter for Lasso\n",
    "l_l2 = 0.5    # regularation parameter for Ridge\n",
    "\n",
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(x)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(prox_lasso(x, s=l_l1))\n",
    "plt.title(\"Proximal Lasso\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(prox_ridge(x, s=l_l2))\n",
    "plt.title(\"Proximal Ridge\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Question</b>:\n",
    "     <ul>\n",
    "      <li>Comment what you observe (1 or 2 sentences).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original parameters are 50 random numbers from the “standard normal” distribution. \n",
    "\n",
    "The proximity operator for Lasso has the effect to regularize the parameter, when the absolute value of the original parameter is smaller than the regularation parameter $s$ (where $s = 1$ in this case), the parameter is reduced to the zero, and when the absolute value of the original parameter is bigger than $s$, its absolute value is reduced by $s$, with the sign unchanged. \n",
    "\n",
    "For the proximity operator for Ridge, the orignal parameter is divided by $(1. + s)$, where $s = 0.5$ in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradients\n",
    "\n",
    "The problems we want to minimize take the form:\n",
    "$$\n",
    "\\arg\\min_x f(x) + g(x)\n",
    "$$\n",
    "where $f$ is $L$-smooth and $g$ is prox-capable.\n",
    "\n",
    "Consider the following cases:\n",
    "\n",
    "**Linear regression**, where \n",
    "$$\n",
    "f(x) = \\frac{1}{2n} \\sum_{i=1}^n (b_i - a_i^\\top x)^2 = \\frac{1}{2 n} \\| b - A x \\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, $b = [b_1 \\cdots b_n]$ is the vector of labels and $A$ is the matrix of features.\n",
    "\n",
    "**Logistic regression**, where\n",
    "$$\n",
    "f(x) = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + \\exp(-b_i a_i^\\top x)),\n",
    "$$\n",
    "where $n$ is the sample size, and where labels $b_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We need to be able to compute $f$ and its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Questions</b>:\n",
    "     <ul>\n",
    "      <li>Compute on paper the gradient $\\nabla f$ of $f$ for both cases (linear and logistic regression)</li>\n",
    "      <li>Code a function that computes $f$ and its gradient $\\nabla f$ in both cases, using the prototypes below.</li>\n",
    "      <li>Check that these functions are correct by numerically checking the gradient, using the function ``<a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.check_grad.html\">check_grad</a>`` from ``scipy.optimize``. Remark: use the functions `simu_linreg` and `simu_logreg` to simulate data according to the right model</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_linreg(x):\n",
    "    \"\"\" Least-squares loss\"\"\"\n",
    "    \"\"\"\n",
    "    # A: matrix of features\n",
    "    # b: b=[b1⋯bn]is the vector of labels\n",
    "    # x: observations\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1. / (2. * n_samples) * norm(A_lr.dot(x) - b_lr) ** 2.\n",
    "\n",
    "\n",
    "def grad_linreg(x):\n",
    "    \"\"\"Leas-squares gradient\"\"\"\n",
    "\n",
    "    return 1. / n_samples *  A_lr.T.dot(A_lr).dot(x) - 1. / n_samples * b_lr.T.dot(A_lr)\n",
    "\n",
    "\n",
    "def loss_logreg(x):\n",
    "    \"\"\"Logistic loss\"\"\"\n",
    "    \"\"\"\n",
    "    # A: matrix of features, size n * n\n",
    "    # b:  labels,  bi∈{−1,1} for all i.\n",
    "    # x: observations, len = n (n*1)  \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(1. / n_samples * np.log(1. + np.exp(-b_log * A_log.dot(x))))\n",
    "    \n",
    "\n",
    "def grad_logreg(x):\n",
    "    \"\"\"Logistic gradient\"\"\"\n",
    "    \n",
    "    bAT = np.multiply(A_log, b_log[:, np.newaxis])\n",
    "    sigmoid_part =  1. / (1. + np.exp(bAT.dot(x)))\n",
    "    return np.sum(1. / n_samples * np.multiply(-bAT, sigmoid_part[:, np.newaxis]), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square root of the sum of square of the differences for linear regression:  2.336E-06\n",
      "Square root of the sum of square of the differences for log regression:  1.258E-07\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check that these functions are correct by numerically checking the gradient\"\"\"\n",
    "\n",
    "# check function gradient\n",
    "import scipy.optimize as optimize\n",
    "ck_grad = optimize.check_grad\n",
    "\n",
    "## check for linear regression\n",
    "coefs_lr = coefs[:]\n",
    "x_lr = x[:]\n",
    "A_lr, b_lr = simu_linreg(coefs)\n",
    "\n",
    "# check the function works\n",
    "loss_linreg(x_lr)\n",
    "grad_linreg(x_lr)\n",
    "# check the gradient\n",
    "linear_ck_grad = ck_grad(loss_linreg, grad_linreg, x_lr)\n",
    "print ('Square root of the sum of square of the differences for linear regression: ', \"%.3E\" % linear_ck_grad)\n",
    "\n",
    "\n",
    "## check for log regression\n",
    "x_log = x[:]\n",
    "coefs_log = coefs[:]\n",
    "A_log, b_log = simu_logreg(coefs_log)\n",
    "\n",
    "# check the functions\n",
    "sum_f = loss_logreg(x_log)\n",
    "sum_df = grad_logreg(x_log)\n",
    "log_ck_grads = ck_grad(loss_logreg, grad_logreg, x_log)\n",
    "\n",
    "print ('Square root of the sum of square of the differences for log regression: ', \"%.3E\" % log_ck_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know have a function to compute $f$, $\\nabla f$ and $g$ and $\\text{prox}_g$. \n",
    "\n",
    "We want now to code the Ista and Fista solvers to minimize\n",
    "\n",
    "$$\n",
    "\\arg\\min_x f(x) + g(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Questions</b>:\n",
    "     <ul>\n",
    "      <li>Implement functions that compute the Lipschitz constants for linear and \n",
    "  logistic regression losses. Note that the operator norm of a matrix can \n",
    "  be computed using the function <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html\">numpy.linalg.norm</a> (read the documentation\n",
    "  of the function)</li>\n",
    "      <li>Finish the functions `ista` and `fista` below that implements the \n",
    "  ISTA (Proximal Gradient Descent) and FISTA (Accelerated Proximal \n",
    "  Gradient Descent) algorithms</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lip_linreg(A):\n",
    "    \"\"\"Lipschitz constant for linear squares loss\"\"\"    \n",
    "    \"\"\" # f(x) second order derivative  = 1 / n * A.T * A  \"\"\"\n",
    "    \n",
    "    return norm(A, ord=2) ** 2. / n_samples\n",
    "    \n",
    "def lip_logreg(A):\n",
    "    \"\"\"Lipschitz constant for logistic loss\"\"\"\n",
    "    \n",
    "    return norm(A, ord=2) ** 2. / 4.\n",
    "       \n",
    "def ista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50,\n",
    "         x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = norm(x - x_true) / norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = f(x) + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching ISTA solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "                    \n",
    "    for k in range(n_iter + 1):        \n",
    "         \n",
    "        x = prox_g(x - step * grad_f(x), s)       \n",
    "        obj = f(x) + g(x, s)\n",
    "        err = norm(x - x_true) / norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "     \n",
    "        if k % 100 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50,\n",
    "         x_true=coefs, verbose=True):\n",
    "    \"\"\"Accelerated Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    # An extra variable is required for FISTA\n",
    "    z = x0.copy()\n",
    "    # n_samples, n_features = A.shape\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = norm(x - x_true) / norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = f(x) + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    t = 1.\n",
    "    t_new = 1.   \n",
    "    \n",
    "    beta = 1\n",
    "    if verbose:\n",
    "        print(\"Lauching FISTA solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter + 1):\n",
    "             \n",
    "        xplus = prox_g(z - step * grad_f(z), s)\n",
    "        betaplus = (1. + (1. + 4. * beta ** 2.) ** 0.5) / 2.\n",
    "        z = xplus + (beta - 1.) / betaplus * (xplus - x)\n",
    "        x = xplus\n",
    "        beta = betaplus       \n",
    "\n",
    "        obj = f(x) + g(x, s)\n",
    "        err = norm(x - x_true) / norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "\n",
    "        if k % 100 == 0 and verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lauching FISTA solver...\n",
      "   it    |   obj    |   err   \n",
      "       0 | 1.52e+00 | 9.99e-01\n",
      "     100 | 1.28e+00 | 9.00e-01\n",
      "     200 | 1.28e+00 | 9.02e-01\n",
      "     300 | 1.28e+00 | 9.02e-01\n",
      "     400 | 1.28e+00 | 9.02e-01\n",
      "     500 | 1.28e+00 | 9.02e-01\n",
      "     600 | 1.28e+00 | 9.02e-01\n",
      "     700 | 1.28e+00 | 9.02e-01\n",
      "     800 | 1.28e+00 | 9.02e-01\n",
      "     900 | 1.28e+00 | 9.02e-01\n",
      "    1000 | 1.28e+00 | 9.02e-01\n",
      "    1100 | 1.28e+00 | 9.02e-01\n",
      "    1200 | 1.28e+00 | 9.02e-01\n",
      "    1300 | 1.28e+00 | 9.02e-01\n",
      "    1400 | 1.28e+00 | 9.02e-01\n",
      "    1500 | 1.28e+00 | 9.02e-01\n",
      "    1600 | 1.28e+00 | 9.02e-01\n",
      "    1700 | 1.28e+00 | 9.02e-01\n",
      "    1800 | 1.28e+00 | 9.02e-01\n",
      "    1900 | 1.28e+00 | 9.02e-01\n",
      "    2000 | 1.28e+00 | 9.02e-01\n",
      "    2100 | 1.28e+00 | 9.02e-01\n",
      "    2200 | 1.28e+00 | 9.02e-01\n",
      "    2300 | 1.28e+00 | 9.02e-01\n",
      "    2400 | 1.28e+00 | 9.02e-01\n",
      "    2500 | 1.28e+00 | 9.02e-01\n",
      "    2600 | 1.28e+00 | 9.02e-01\n",
      "    2700 | 1.28e+00 | 9.02e-01\n",
      "    2800 | 1.28e+00 | 9.02e-01\n",
      "    2900 | 1.28e+00 | 9.02e-01\n",
      "    3000 | 1.28e+00 | 9.02e-01\n",
      "    3100 | 1.28e+00 | 9.02e-01\n",
      "    3200 | 1.28e+00 | 9.02e-01\n",
      "    3300 | 1.28e+00 | 9.02e-01\n",
      "    3400 | 1.28e+00 | 9.02e-01\n",
      "    3500 | 1.28e+00 | 9.02e-01\n",
      "    3600 | 1.28e+00 | 9.02e-01\n",
      "    3700 | 1.28e+00 | 9.02e-01\n",
      "    3800 | 1.28e+00 | 9.02e-01\n",
      "    3900 | 1.28e+00 | 9.02e-01\n",
      "    4000 | 1.28e+00 | 9.02e-01\n",
      "    4100 | 1.28e+00 | 9.02e-01\n",
      "    4200 | 1.28e+00 | 9.02e-01\n",
      "    4300 | 1.28e+00 | 9.02e-01\n",
      "    4400 | 1.28e+00 | 9.02e-01\n",
      "    4500 | 1.28e+00 | 9.02e-01\n",
      "    4600 | 1.28e+00 | 9.02e-01\n",
      "    4700 | 1.28e+00 | 9.02e-01\n",
      "    4800 | 1.28e+00 | 9.02e-01\n",
      "    4900 | 1.28e+00 | 9.02e-01\n",
      "    5000 | 1.28e+00 | 9.02e-01\n",
      "    5100 | 1.28e+00 | 9.02e-01\n",
      "    5200 | 1.28e+00 | 9.02e-01\n",
      "    5300 | 1.28e+00 | 9.02e-01\n",
      "    5400 | 1.28e+00 | 9.02e-01\n",
      "    5500 | 1.28e+00 | 9.02e-01\n",
      "    5600 | 1.28e+00 | 9.02e-01\n",
      "    5700 | 1.28e+00 | 9.02e-01\n",
      "    5800 | 1.28e+00 | 9.02e-01\n",
      "    5900 | 1.28e+00 | 9.02e-01\n",
      "    6000 | 1.28e+00 | 9.02e-01\n",
      "    6100 | 1.28e+00 | 9.02e-01\n",
      "    6200 | 1.28e+00 | 9.02e-01\n",
      "    6300 | 1.28e+00 | 9.02e-01\n",
      "    6400 | 1.28e+00 | 9.02e-01\n",
      "    6500 | 1.28e+00 | 9.02e-01\n",
      "    6600 | 1.28e+00 | 9.02e-01\n",
      "    6700 | 1.28e+00 | 9.02e-01\n",
      "    6800 | 1.28e+00 | 9.02e-01\n",
      "    6900 | 1.28e+00 | 9.02e-01\n",
      "    7000 | 1.28e+00 | 9.02e-01\n",
      "    7100 | 1.28e+00 | 9.02e-01\n",
      "    7200 | 1.28e+00 | 9.02e-01\n",
      "    7300 | 1.28e+00 | 9.02e-01\n",
      "    7400 | 1.28e+00 | 9.02e-01\n",
      "    7500 | 1.28e+00 | 9.02e-01\n",
      "    7600 | 1.28e+00 | 9.02e-01\n",
      "    7700 | 1.28e+00 | 9.02e-01\n",
      "    7800 | 1.28e+00 | 9.02e-01\n",
      "    7900 | 1.28e+00 | 9.02e-01\n",
      "    8000 | 1.28e+00 | 9.02e-01\n",
      "    8100 | 1.28e+00 | 9.02e-01\n",
      "    8200 | 1.28e+00 | 9.02e-01\n",
      "    8300 | 1.28e+00 | 9.02e-01\n",
      "    8400 | 1.28e+00 | 9.02e-01\n",
      "    8500 | 1.28e+00 | 9.02e-01\n",
      "    8600 | 1.28e+00 | 9.02e-01\n",
      "    8700 | 1.28e+00 | 9.02e-01\n",
      "    8800 | 1.28e+00 | 9.02e-01\n",
      "    8900 | 1.28e+00 | 9.02e-01\n",
      "    9000 | 1.28e+00 | 9.02e-01\n",
      "    9100 | 1.28e+00 | 9.02e-01\n",
      "    9200 | 1.28e+00 | 9.02e-01\n",
      "    9300 | 1.28e+00 | 9.02e-01\n",
      "    9400 | 1.28e+00 | 9.02e-01\n",
      "    9500 | 1.28e+00 | 9.02e-01\n",
      "    9600 | 1.28e+00 | 9.02e-01\n",
      "    9700 | 1.28e+00 | 9.02e-01\n",
      "    9800 | 1.28e+00 | 9.02e-01\n",
      "    9900 | 1.28e+00 | 9.02e-01\n",
      "   10000 | 1.28e+00 | 9.02e-01\n",
      "   10100 | 1.28e+00 | 9.02e-01\n",
      "   10200 | 1.28e+00 | 9.02e-01\n",
      "   10300 | 1.28e+00 | 9.02e-01\n",
      "   10400 | 1.28e+00 | 9.02e-01\n",
      "   10500 | 1.28e+00 | 9.02e-01\n",
      "   10600 | 1.28e+00 | 9.02e-01\n",
      "   10700 | 1.28e+00 | 9.02e-01\n",
      "   10800 | 1.28e+00 | 9.02e-01\n",
      "   10900 | 1.28e+00 | 9.02e-01\n",
      "   11000 | 1.28e+00 | 9.02e-01\n",
      "   11100 | 1.28e+00 | 9.02e-01\n",
      "   11200 | 1.28e+00 | 9.02e-01\n",
      "   11300 | 1.28e+00 | 9.02e-01\n",
      "   11400 | 1.28e+00 | 9.02e-01\n",
      "   11500 | 1.28e+00 | 9.02e-01\n",
      "   11600 | 1.28e+00 | 9.02e-01\n",
      "   11700 | 1.28e+00 | 9.02e-01\n",
      "   11800 | 1.28e+00 | 9.02e-01\n",
      "   11900 | 1.28e+00 | 9.02e-01\n",
      "   12000 | 1.28e+00 | 9.02e-01\n",
      "   12100 | 1.28e+00 | 9.02e-01\n",
      "   12200 | 1.28e+00 | 9.02e-01\n",
      "   12300 | 1.28e+00 | 9.02e-01\n",
      "   12400 | 1.28e+00 | 9.02e-01\n",
      "   12500 | 1.28e+00 | 9.02e-01\n",
      "   12600 | 1.28e+00 | 9.02e-01\n",
      "   12700 | 1.28e+00 | 9.02e-01\n",
      "   12800 | 1.28e+00 | 9.02e-01\n",
      "   12900 | 1.28e+00 | 9.02e-01\n",
      "   13000 | 1.28e+00 | 9.02e-01\n",
      "   13100 | 1.28e+00 | 9.02e-01\n",
      "   13200 | 1.28e+00 | 9.02e-01\n",
      "   13300 | 1.28e+00 | 9.02e-01\n",
      "   13400 | 1.28e+00 | 9.02e-01\n",
      "   13500 | 1.28e+00 | 9.02e-01\n",
      "   13600 | 1.28e+00 | 9.02e-01\n",
      "   13700 | 1.28e+00 | 9.02e-01\n",
      "   13800 | 1.28e+00 | 9.02e-01\n",
      "   13900 | 1.28e+00 | 9.02e-01\n",
      "   14000 | 1.28e+00 | 9.02e-01\n",
      "   14100 | 1.28e+00 | 9.02e-01\n",
      "   14200 | 1.28e+00 | 9.02e-01\n",
      "   14300 | 1.28e+00 | 9.02e-01\n",
      "   14400 | 1.28e+00 | 9.02e-01\n",
      "   14500 | 1.28e+00 | 9.02e-01\n",
      "   14600 | 1.28e+00 | 9.02e-01\n",
      "   14700 | 1.28e+00 | 9.02e-01\n",
      "   14800 | 1.28e+00 | 9.02e-01\n",
      "   14900 | 1.28e+00 | 9.02e-01\n",
      "   15000 | 1.28e+00 | 9.02e-01\n",
      "   15100 | 1.28e+00 | 9.02e-01\n",
      "   15200 | 1.28e+00 | 9.02e-01\n",
      "   15300 | 1.28e+00 | 9.02e-01\n",
      "   15400 | 1.28e+00 | 9.02e-01\n",
      "   15500 | 1.28e+00 | 9.02e-01\n",
      "   15600 | 1.28e+00 | 9.02e-01\n",
      "   15700 | 1.28e+00 | 9.02e-01\n",
      "   15800 | 1.28e+00 | 9.02e-01\n",
      "   15900 | 1.28e+00 | 9.02e-01\n",
      "   16000 | 1.28e+00 | 9.02e-01\n",
      "   16100 | 1.28e+00 | 9.02e-01\n",
      "   16200 | 1.28e+00 | 9.02e-01\n",
      "   16300 | 1.28e+00 | 9.02e-01\n",
      "   16400 | 1.28e+00 | 9.02e-01\n",
      "   16500 | 1.28e+00 | 9.02e-01\n",
      "   16600 | 1.28e+00 | 9.02e-01\n",
      "   16700 | 1.28e+00 | 9.02e-01\n",
      "   16800 | 1.28e+00 | 9.02e-01\n",
      "   16900 | 1.28e+00 | 9.02e-01\n",
      "   17000 | 1.28e+00 | 9.02e-01\n",
      "   17100 | 1.28e+00 | 9.02e-01\n",
      "   17200 | 1.28e+00 | 9.02e-01\n",
      "   17300 | 1.28e+00 | 9.02e-01\n",
      "   17400 | 1.28e+00 | 9.02e-01\n",
      "   17500 | 1.28e+00 | 9.02e-01\n",
      "   17600 | 1.28e+00 | 9.02e-01\n",
      "   17700 | 1.28e+00 | 9.02e-01\n",
      "   17800 | 1.28e+00 | 9.02e-01\n",
      "   17900 | 1.28e+00 | 9.02e-01\n",
      "   18000 | 1.28e+00 | 9.02e-01\n",
      "   18100 | 1.28e+00 | 9.02e-01\n",
      "   18200 | 1.28e+00 | 9.02e-01\n",
      "   18300 | 1.28e+00 | 9.02e-01\n",
      "   18400 | 1.28e+00 | 9.02e-01\n",
      "   18500 | 1.28e+00 | 9.02e-01\n",
      "   18600 | 1.28e+00 | 9.02e-01\n",
      "   18700 | 1.28e+00 | 9.02e-01\n",
      "   18800 | 1.28e+00 | 9.02e-01\n",
      "   18900 | 1.28e+00 | 9.02e-01\n",
      "   19000 | 1.28e+00 | 9.02e-01\n",
      "   19100 | 1.28e+00 | 9.02e-01\n",
      "   19200 | 1.28e+00 | 9.02e-01\n",
      "   19300 | 1.28e+00 | 9.02e-01\n",
      "   19400 | 1.28e+00 | 9.02e-01\n",
      "   19500 | 1.28e+00 | 9.02e-01\n",
      "   19600 | 1.28e+00 | 9.02e-01\n",
      "   19700 | 1.28e+00 | 9.02e-01\n",
      "   19800 | 1.28e+00 | 9.02e-01\n",
      "   19900 | 1.28e+00 | 9.02e-01\n",
      "   20000 | 1.28e+00 | 9.02e-01\n",
      "   20100 | 1.28e+00 | 9.02e-01\n",
      "   20200 | 1.28e+00 | 9.02e-01\n",
      "   20300 | 1.28e+00 | 9.02e-01\n",
      "   20400 | 1.28e+00 | 9.02e-01\n",
      "   20500 | 1.28e+00 | 9.02e-01\n",
      "   20600 | 1.28e+00 | 9.02e-01\n",
      "   20700 | 1.28e+00 | 9.02e-01\n",
      "   20800 | 1.28e+00 | 9.02e-01\n",
      "   20900 | 1.28e+00 | 9.02e-01\n",
      "   21000 | 1.28e+00 | 9.02e-01\n",
      "   21100 | 1.28e+00 | 9.02e-01\n",
      "   21200 | 1.28e+00 | 9.02e-01\n",
      "   21300 | 1.28e+00 | 9.02e-01\n",
      "   21400 | 1.28e+00 | 9.02e-01\n",
      "   21500 | 1.28e+00 | 9.02e-01\n",
      "   21600 | 1.28e+00 | 9.02e-01\n",
      "   21700 | 1.28e+00 | 9.02e-01\n",
      "   21800 | 1.28e+00 | 9.02e-01\n",
      "   21900 | 1.28e+00 | 9.02e-01\n",
      "   22000 | 1.28e+00 | 9.02e-01\n",
      "   22100 | 1.28e+00 | 9.02e-01\n",
      "   22200 | 1.28e+00 | 9.02e-01\n",
      "   22300 | 1.28e+00 | 9.02e-01\n",
      "   22400 | 1.28e+00 | 9.02e-01\n",
      "   22500 | 1.28e+00 | 9.02e-01\n",
      "   22600 | 1.28e+00 | 9.02e-01\n",
      "   22700 | 1.28e+00 | 9.02e-01\n",
      "   22800 | 1.28e+00 | 9.02e-01\n",
      "   22900 | 1.28e+00 | 9.02e-01\n",
      "   23000 | 1.28e+00 | 9.02e-01\n",
      "   23100 | 1.28e+00 | 9.02e-01\n",
      "   23200 | 1.28e+00 | 9.02e-01\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Compute a precise minimum and a precise minimizer \"\"\"\n",
    "\n",
    "# Some definitions before launching the algorithms\n",
    "x0 = np.zeros(n_features)\n",
    "n_iter = 30000\n",
    "A_lr, b_lr = simu_linreg(coefs)\n",
    "L = lip_linreg(A_lr)\n",
    "s = 1e-2\n",
    "step = s / L\n",
    "f = loss_linreg\n",
    "grad_f = grad_linreg\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "\n",
    "fista_x, fista_objectives, fista_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=True)\n",
    "\n",
    "print (\"\\n\")\n",
    "# Use the same setting of fista to call ista again\n",
    "ista_x, ista_objectives, ista_errors = ista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=True)\n",
    "\n",
    "print (\"\\n\")\n",
    "print ('The precise minimum is: ',  fista_objectives[-1])\n",
    "print ('The precise minimizer is: ',  fista_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" display the result with fista method\"\"\"\n",
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(fista_x)\n",
    "plt.ylabel('x value')\n",
    "plt.title(\"(final) x value (minimizer), fista\", fontsize=16)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(fista_objectives)\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('objective function value')\n",
    "plt.title(\"objectives (minimum), fista\", fontsize=16)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(fista_errors)\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('error value')\n",
    "plt.title(\"Errors, fista\", fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the convergences of ISTA and FISTA, in terms of distance to the minimum and distance to the minimizer. Do your plots using a logarithmic scale of the y-axis.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Compare the convergences of ISTA and FISTA \"\"\"\n",
    "\n",
    "iteration_ids = np.arange(0, n_iter + 2)\n",
    "plt.figure(figsize=(15., 8.))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(iteration_ids, fista_objectives, label='fista_objective')\n",
    "plt.plot(iteration_ids, ista_objectives, label='ista_objective')\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('objective values',fontsize=14)\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.title('Comparison of objective values between fista and ista method',fontsize=18)\n",
    "\n",
    "\n",
    "minimun_f = fista_objectives[-1]\n",
    "\n",
    "# compare the convergences of ISTA and FISTA in terms of distance to the minimum \n",
    "fista_dis_minimum = (np.abs(fista_objectives - minimun_f) ** 2.) ** (1./2)\n",
    "ista_dis_minimum = (np.abs(ista_objectives - minimun_f) ** 2.) ** (1./2) \n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.semilogy(iteration_ids, fista_dis_minimum, label=\"fista_distance\")\n",
    "plt.semilogy(iteration_ids, ista_dis_minimum, label=\"ista_distance\")\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('distance to the minimum in log-scale', fontsize=14)\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.title('Comparison of distance to the minimum between fista and ista method', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fista coverges to the solution much faster than the ista method. \n",
    "### Question to the TA: should the distance to the minimizer be considered? if yes, then in each step, the x value should be stored in fista and ista function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>Compare the solution you obtain with ista and fista with the true parameter `coefs` of\n",
    "  the model. This can be done with `plt.stem` plots.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "This will be reffered as Q0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************** Q0 *************************\n",
    "\"\"\" Compare the solution of fista and ista with true coefs \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(18., 6.))\n",
    "grid = plt.GridSpec(2, 90, wspace=0.4, hspace=0.3)\n",
    "\n",
    "plt.subplot(grid[0, 0:26])\n",
    "plt.stem(fista_x)\n",
    "plt.ylabel('x (minimizer) value',  fontsize=14)\n",
    "plt.title('x (minimizer) value, fista', fontsize=18)\n",
    "plt.subplot(grid[0, 32:58])\n",
    "plt.stem(ista_x)\n",
    "plt.ylabel('x (minimizer) value',  fontsize=14)\n",
    "plt.title('x (minimizer) value, ista', fontsize=18)\n",
    "\n",
    "plt.subplot(grid[0, 64:90])\n",
    "plt.stem(coefs)\n",
    "plt.ylabel('true x (coefs) vlaue', fontsize=14)\n",
    "plt.title('true x (coefs) vlaue', fontsize=18)\n",
    "\n",
    "plt.subplot(grid[1, 0:44])\n",
    "plt.scatter(fista_x,coefs)\n",
    "plt.xlabel('x (minimizer) value, fista', fontsize=14)\n",
    "plt.ylabel('true x (coefs) vlaue', fontsize=14)\n",
    "plt.title('x (minimizer, fista) vs true x(coefs)', fontsize=18)\n",
    "\n",
    "plt.subplot(grid[1, 46:90])\n",
    "plt.scatter(ista_x,coefs)\n",
    "plt.xlabel('x (minimizer) value, ista', fontsize=14)\n",
    "plt.ylabel('true x (coefs) vlaue', fontsize=14)\n",
    "plt.title(' x (minimizer, ista) vs true x(coefs)', fontsize=18)\n",
    "fig.get_axes()[0].annotate('Compare the solution of fista and ista with true coefs',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print ('The error between fista_x and ista_x: \\n', fista_x - ista_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the true value, the minimizer is much smaller on absolute level for those whose true values are large, and the minimizer almost does not reach zero for those whose true values are zero. The fista and ista method return almost the same x_value (minimizer). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>Compare the solution you obtain with ista and fista with the true parameter `coefs` of\n",
    "  the model. This can be done with `plt.stem` plots.</li>\n",
    "</div>\n",
    "This will be referred as Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In linear regression and logistic regression, study the influence of the correlation of the features\n",
    "# on the performance of the optimization algorithms. Explain.\n",
    "#******************* Q1 ***********************************************\n",
    "\n",
    "\"\"\"  We first check the influence on the linear regression:  Part 1 \"\"\"\n",
    "\"\"\"  Then we check the influence on the logistic regression: Part 2 \"\"\"\n",
    "\"\"\"  Finally, we comment and explain the result! :           Part 3 \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Part 1: correlation influence on the linear regresison (with fista method)\"\"\"\n",
    "\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "linecycler = cycle(lines)\n",
    "colors = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
    "\n",
    "# we test the correlation with seven differnt values\n",
    "correlations = [-1, -0.7, -0.3, 0, 0.3, 0.7, 1]\n",
    "\n",
    "# the settings are the same as above, only the number of iterations is reduced to 150\n",
    "x0 = np.zeros(n_features)\n",
    "n_iter = 150\n",
    "s = 1e-2\n",
    "f = loss_linreg\n",
    "grad_f = grad_linreg\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "\n",
    "iteration_ids = np.arange(0, n_iter + 2)\n",
    "nr_corrs = len(correlations)\n",
    "nr_f_obj = len(iteration_ids)\n",
    "f_obj = np.zeros(shape=(nr_f_obj,nr_corrs))\n",
    "xs = np.zeros((n_features, nr_corrs))\n",
    "errors = np.zeros((nr_f_obj, nr_corrs))\n",
    "\n",
    "### calculate new results with different correlations\n",
    "for i in range(nr_corrs):\n",
    "    corri = correlations[i]\n",
    "    A_lr, b_lr = simu_linreg(coefs, corr=corri)\n",
    "    L = lip_linreg(A_lr)\n",
    "    step = s / L\n",
    "    fista_x, fista_objectives, fista_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "    f_obj[:,i] = fista_objectives   \n",
    "    xs[:,i] = fista_x\n",
    "    errors[:,i] = fista_errors\n",
    "\n",
    " \n",
    "### plot the objective values for different correlations\n",
    "fig = plt.figure(figsize=(18., 9.))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(nr_corrs):  \n",
    "    corri = correlations[i]\n",
    "    plt.plot(iteration_ids, f_obj[:,i], next(linecycler),  \n",
    "             color=next(colors), label=\"corri=\"+str(round(corri,2)))\n",
    "plt.legend(loc=1,fontsize=14)\n",
    "plt.xlabel('number of iterations',fontsize=14)\n",
    "plt.ylabel('obj values', fontsize=14)\n",
    "plt.title('objective value for different correlations',fontsize=18)\n",
    "plt.ylim([0, 15])\n",
    "\n",
    "### plot the error value for differnt correlations\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(nr_corrs):  \n",
    "    corri = correlations[i]\n",
    "    plt.plot(iteration_ids, errors[:,i],next(linecycler),color=next(colors),\n",
    "             label=\"corri=\"+str(round(corri,2)))\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('error values',fontsize=14)\n",
    "plt.title('error value for different correlations', fontsize=18)\n",
    "fig.get_axes()[0].annotate('Correlation influence on the convergence rate of linear regression',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly, from the picture we see that for the fista method for linear regression,  the higher the correlation, the faster the covergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************* Q1 ****************************************************\n",
    "\n",
    "### plot x (minimizer) values for different correlations: linear regression\n",
    "fig = plt.figure(figsize=(12, 20))\n",
    "for i in range(nr_corrs):  \n",
    "    corri = correlations[i]    \n",
    "    ax = fig.add_subplot(nr_corrs,1,i+1)\n",
    "    ax.stem(xs[:,i], label=\"corri=\"+str(round(corri,2)))    \n",
    "    ax.legend(loc=1, fontsize=14)\n",
    "    ax.set_ylabel('x values',fontsize=14)\n",
    "    \n",
    "fig.get_axes()[0].annotate('x(minimizer)values for differnt correlations: linear regression',\n",
    "                           (0.5, 0.94), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When the absolute level of the correlation is not too high, the x(minimizer) value still resembles the true coefs value. However when the absolute level of the correlation approaches one, the minimizer differ from the coefs value significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************* Q1 ****************************************************\n",
    "\"\"\"Part 2: correlation influence on the logistic regresison (with fista method)\"\"\"\n",
    "\n",
    "# the settings are the same as above, only the number of iterations is reduced to 150\n",
    "x0 = np.zeros(n_features)\n",
    "n_iter = 150\n",
    "s = 1e-2\n",
    "f = loss_logreg\n",
    "grad_f = grad_logreg\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "\n",
    "iteration_ids = np.arange(0, n_iter + 2)\n",
    "nr_corrs = len(correlations)\n",
    "nr_f_obj = len(iteration_ids)\n",
    "f_obj_log = np.zeros(shape=(nr_f_obj,nr_corrs))\n",
    "xs_log = np.zeros((n_features, nr_corrs))\n",
    "errors_log = np.zeros((nr_f_obj, nr_corrs))\n",
    "\n",
    "### calculate new results with different correlations\n",
    "for i in range(nr_corrs):\n",
    "    corri = correlations[i]\n",
    "    A_log, b_log = simu_logreg(coefs, corr=corri)\n",
    "    L = lip_logreg(A_log)\n",
    "    step = s / L\n",
    "    fista_x, fista_objectives, fista_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "    f_obj_log[:,i] = fista_objectives   \n",
    "    xs_log[:,i] = fista_x\n",
    "    errors_log[:,i] = fista_errors\n",
    "\n",
    " \n",
    "### plot the objective values for different correlations\n",
    "fig = plt.figure(figsize=(18., 9.))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(nr_corrs):  \n",
    "    corri = correlations[i]\n",
    "    plt.plot(iteration_ids, f_obj_log[:,i], next(linecycler),  \n",
    "             color=next(colors), label=\"corri=\"+str(round(corri,2)))\n",
    "plt.legend(loc=1,fontsize=14)\n",
    "plt.xlabel('number of iterations',fontsize=14)\n",
    "plt.ylabel('obj values', fontsize=14)\n",
    "plt.title('objective value for different correlations',fontsize=18)\n",
    "\n",
    "\n",
    "### plot the error value for differnt correlations\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(nr_corrs):  \n",
    "    corri = correlations[i]\n",
    "    plt.plot(iteration_ids, errors_log[:,i],next(linecycler),color=next(colors),\n",
    "             label=\"corri=\"+str(round(corri,2)))\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('error values',fontsize=14)\n",
    "plt.title('error value for different correlations', fontsize=18)\n",
    "fig.get_axes()[0].annotate('Correlation influence on the convergence rate of logistic regression',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly, from the picture we see that for the fista method for logistic regression,  the higher the correlation, the faster the covergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************* Q1 ****************************************************\n",
    "### plot x (minimizer) values for different correlations: logistic regression\n",
    "fig = plt.figure(figsize=(12, 20))\n",
    "for i in range(nr_corrs):  \n",
    "    corri = correlations[i]    \n",
    "    ax = fig.add_subplot(nr_corrs,1,i+1)\n",
    "    ax.stem(xs_log[:,i], label=\"corri=\"+str(round(corri,2)))    \n",
    "    ax.legend(loc=1, fontsize=14)\n",
    "    ax.set_ylabel('x values',fontsize=14)\n",
    "    \n",
    "fig.get_axes()[0].annotate('x(minimizer)values for differnt correlations: logistic regression',\n",
    "                           (0.5, 0.94), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commmet for Q1 (study the influence of the correlation ): \n",
    "For logistic regression,  when the absolute level of the correlation is not too high, the x(minimizer) value still resembles the true coefs value. However when the absolute level of the correlation approaches one, the minimizer differ from the coefs value significantly.\n",
    "\n",
    "Togeter with the figures before, clearly, for both linear and logistic regression, the smaller the correlation, the faster the convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "    <li>In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "This will be referred as Q2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************** Q2 ************************************************\n",
    "\"\"\" the level of ridge penalization on the performance of the optimization algorithms\"\"\"\n",
    "\"\"\" For linear regression, fista method \"\"\"\n",
    "\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "linecycler = cycle(lines)\n",
    "colors = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
    "\n",
    "# we test the level of ridge penalizaiton with seven differnt values\n",
    "s_levels = [10, 5, 1, 0.5, 0.05, 1e-3, 1e-4]\n",
    "x0 = np.zeros(n_features)\n",
    "n_iter = 600\n",
    "\n",
    "f = loss_linreg\n",
    "grad_f = grad_linreg\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "\n",
    "iteration_ids = np.arange(0, n_iter + 2)\n",
    "nr_tests = len(s_levels)\n",
    "nr_f_obj = len(iteration_ids)\n",
    "f_obj = np.zeros(shape=(nr_f_obj,nr_corrs))\n",
    "xs = np.zeros((n_features, nr_tests))\n",
    "errors = np.zeros((nr_f_obj, nr_tests))\n",
    "\n",
    "### calculate new results with different correlations\n",
    "for i in range(nr_tests):\n",
    "    s = s_levels[i]\n",
    "    A_lr, b_lr = simu_linreg(coefs)\n",
    "    L = lip_linreg(A_lr)\n",
    "    step = s / L\n",
    "    fista_x, fista_objectives, fista_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "    f_obj[:,i] = fista_objectives   \n",
    "    xs[:,i] = fista_x\n",
    "    errors[:,i] = fista_errors\n",
    "\n",
    " \n",
    "### plot the objective values for different penalty level\n",
    "fig = plt.figure(figsize=(18., 9.))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(2,nr_tests):  \n",
    "    s = s_levels[i]\n",
    "    plt.plot(iteration_ids, f_obj[:,i], next(linecycler),  \n",
    "             color=next(colors), label=\"penaty_level=\"+str(s))\n",
    "plt.legend(loc=1,fontsize=14)\n",
    "plt.xlabel('number of iterations',fontsize=14)\n",
    "plt.ylabel('obj values', fontsize=14)\n",
    "plt.title('objective value for different penaty level',fontsize=18)\n",
    "#plt.ylim([0, ])\n",
    "\n",
    "### plot the error value for differnt penalty level\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(2, nr_tests):  \n",
    "    s = s_levels[i]\n",
    "    plt.plot(iteration_ids, errors[:,i],next(linecycler),color=next(colors),\n",
    "             label=\"penaty_level=\"+str(s))\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('error values',fontsize=14)\n",
    "plt.title('error value for different penaty level', fontsize=18)\n",
    "fig.get_axes()[0].annotate('penaty level influence on the convergence rate of linear regression',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************** Q2 ************************************************\n",
    "\n",
    "We only plot the when penalty level is smaller than one. For the case when the penaty level is greater than one,  the higher the number, the more meanningless the result becomes. And as the figure above shows, the smaller the penaty, the smaller the error becomes and the more accurate the result is. However when the penaty level decreases, the number of iterations required for achieving covergence increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************** Q2 ************************************************\n",
    "### plot x (minimizer) values for different correlations: linear regression\n",
    "fig = plt.figure(figsize=(12, 20))\n",
    "for i in range(nr_tests):  \n",
    "    si = s_levels[i]    \n",
    "    ax = fig.add_subplot(nr_tests,1,i+1)\n",
    "    ax.stem(xs[:,i], label=\"penaty_level=\"+str(si))    \n",
    "    ax.legend(loc=1, fontsize=14)\n",
    "    ax.set_ylabel('x values',fontsize=14)\n",
    "    \n",
    "fig.get_axes()[0].annotate('x(minimizer)values for different penaty level: linear regression, fista method',\n",
    "                           (0.5, 0.94), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************** Q2 ************************************************\n",
    "\"\"\" the level of ridge penalization on the performance of the optimization algorithms\"\"\"\n",
    "\"\"\" For logistic regression, fista method \"\"\"\n",
    "\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "linecycler = cycle(lines)\n",
    "colors = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
    "\n",
    "# we test the level of ridge penalizaiton with seven differnt values\n",
    "s_levels = [10, 5, 1, 0.5, 0.05, 1e-3, 1e-4]\n",
    "x0 = np.zeros(n_features)\n",
    "n_iter = 600\n",
    "\n",
    "f = loss_logreg\n",
    "grad_f = grad_logreg\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "\n",
    "iteration_ids = np.arange(0, n_iter + 2)\n",
    "nr_tests = len(s_levels)\n",
    "nr_f_obj = len(iteration_ids)\n",
    "f_obj = np.zeros(shape=(nr_f_obj,nr_corrs))\n",
    "xs = np.zeros((n_features, nr_tests))\n",
    "errors = np.zeros((nr_f_obj, nr_tests))\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{: 0.5f}'.format})\n",
    "### calculate new results with different correlations\n",
    "for i in range(nr_tests):\n",
    "    s = s_levels[i]\n",
    "    A_lr, b_lr = simu_logreg(coefs)\n",
    "    L = lip_logreg(A_lr)\n",
    "    step = s / L\n",
    "    fista_x, fista_objectives, fista_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "    f_obj[:,i] = fista_objectives \n",
    "    #print ( fista_x, '\\n')    \n",
    "    #print (fista_objectives , '\\n')\n",
    "    xs[:,i] = fista_x\n",
    "    errors[:,i] = fista_errors\n",
    "\n",
    " \n",
    "### plot the objective values for different penalty level\n",
    "fig = plt.figure(figsize=(18., 9.))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(nr_tests):  \n",
    "    s = s_levels[i]\n",
    "    plt.plot(iteration_ids, f_obj[:,i], next(linecycler),  \n",
    "             color=next(colors), label=\"penaty_level=\"+str(s))\n",
    "plt.legend(loc=1,fontsize=14)\n",
    "plt.xlabel('number of iterations',fontsize=14)\n",
    "plt.ylabel('obj values', fontsize=14)\n",
    "plt.title('objective value for different penaty level',fontsize=18)\n",
    "#plt.ylim([0, ])\n",
    "\n",
    "### plot the error value for differnt penalty level\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(nr_tests):  \n",
    "    s = s_levels[i]\n",
    "    plt.plot(iteration_ids, errors[:,i],next(linecycler),color=next(colors),\n",
    "             label=\"penaty_level=\"+str(s))\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('error values',fontsize=14)\n",
    "plt.title('error value for different penaty level', fontsize=18)\n",
    "fig.get_axes()[0].annotate('penaty level influence on the convergence rate of logistc regression, fista method',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************** Q2 ************************************************\n",
    "\n",
    "On first sight, it seems that the smaller the penalty, the more accurate result and the slower the convergence.  However, if we pay attention to the y-lable of each graph, we could see that the objective values and error values for differnt penaty levels (differing in very large scale) are very close. We could basically conclude that the penalty level does not have much influence on teh convergence for the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************* Q2 ****************************************\n",
    "### plot x (minimizer) values for different correlations: logistic regression\n",
    "fig = plt.figure(figsize=(12, 20))\n",
    "for i in range(nr_tests):  \n",
    "    si = s_levels[i]    \n",
    "    ax = fig.add_subplot(nr_tests,1,i+1)\n",
    "    ax.stem(xs[:,i], label=\"penaty_level=\"+str(si))    \n",
    "    ax.legend(loc=1, fontsize=14)\n",
    "    ax.set_ylabel('x values',fontsize=14)\n",
    "    \n",
    "fig.get_axes()[0].annotate('x(minimizer)values for differnt penaty level: logistic regression, fista method',\n",
    "                           (0.5, 0.94), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment for Q2 (the influence of the level of ridge penalization on the performance)\n",
    "\n",
    "Again, the x(minimizer) value are almost identical and almost close to zerofor different pentaly level. This confirms our conclusion that penalty level does not affect the convergence of logistic regression.\n",
    "\n",
    "In summary, penalty level plays a significant role in linear regression, but not so much in logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "    <li>In linear regression and logistic regression, compare the performance of the optimization\n",
    "  algorithms for ridge and lasso penalizations. Explain</li>\n",
    "    </ul>\n",
    "</div>\n",
    "This will be referred as Q3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************** Q3 *******************************************\n",
    "\"\"\"In linear regression and logistic regression, compare the performance of the \n",
    "optimization algorithms for ridge and lasso penalizations. \"\"\"\n",
    "\n",
    "x0 = np.zeros(n_features)\n",
    "s = 1e-2\n",
    "n_iter = 300\n",
    "nr_tests = 4\n",
    "iteration_ids = np.arange(0, n_iter + 2)\n",
    "nr_f_obj = len(iteration_ids)\n",
    "\n",
    "f_obj = np.zeros(shape=(nr_f_obj,nr_tests))\n",
    "xs = np.zeros((n_features, nr_tests))\n",
    "errors = np.zeros((nr_f_obj, nr_tests))\n",
    "\n",
    "# calculate for linear regression\n",
    "f = loss_linreg\n",
    "grad_f = grad_linreg\n",
    "A_lr, b_lr = simu_linreg(coefs)\n",
    "L = lip_linreg(A_lr)\n",
    "step = s / L\n",
    "\n",
    "# calculate linear regression with ridge\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "lin_ridge_x, lin_ridge_objectives, lin_ridge_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "xs[:,0] = lin_ridge_x\n",
    "f_obj[:,0] = lin_ridge_objectives\n",
    "errors[:,0] = lin_ridge_errors\n",
    "\n",
    "# calculate linear regression with lasso\n",
    "g = lasso\n",
    "prox_g = prox_lasso\n",
    "lin_lasso_x, lin_lasso_objectives, lin_lasso_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "\n",
    "\n",
    "xs[:,1] = lin_lasso_x\n",
    "f_obj[:,1] = lin_lasso_objectives\n",
    "errors[:,1] = lin_lasso_errors\n",
    "\n",
    "\n",
    "\n",
    "# calculate for logistic regression\n",
    "f = loss_logreg\n",
    "grad_f = grad_logreg\n",
    "A_log, b_log = simu_logreg(coefs)\n",
    "L = lip_logreg(A_log)\n",
    "step = s / L\n",
    "\n",
    "# calculate  logistic regression with ridge\n",
    "g = ridge\n",
    "prox_g = prox_ridge\n",
    "log_ridge_x, log_ridge_objectives, log_ridge_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "\n",
    "xs[:,2] = log_ridge_x\n",
    "f_obj[:,2] = log_ridge_objectives\n",
    "errors[:,2] = log_ridge_errors\n",
    "\n",
    "\n",
    "# calculate  logistic regression with lasso\n",
    "g = lasso\n",
    "prox_g = prox_lasso\n",
    "log_lasso_x, log_lasso_objectives, log_lasso_errors = fista(x0, f, grad_f, g, prox_g, step, s, n_iter,\n",
    "         x_true=coefs, verbose=False)  \n",
    "\n",
    "xs[:,3] = log_lasso_x\n",
    "f_obj[:,3] = log_lasso_objectives\n",
    "errors[:,3] = log_lasso_errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['linear_rigde', 'linear_lasso', 'log_ridge', 'log_lasso']\n",
    "\n",
    "fig = plt.figure(figsize=(18., 9.))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(2):  \n",
    "    plt.plot(iteration_ids, f_obj[:,i], next(linecycler),  \n",
    "             color=next(colors), label=labels[i])\n",
    "plt.legend(loc=1,fontsize=14)\n",
    "plt.xlabel('number of iterations',fontsize=14)\n",
    "plt.ylabel('obj values', fontsize=14)\n",
    "plt.title('objective value for different penaty level',fontsize=18)\n",
    "#plt.ylim([0, ])\n",
    "\n",
    "### plot the error value for differnt penalty level\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(2):  \n",
    "    plt.plot(iteration_ids, errors[:,i],next(linecycler),color=next(colors),\n",
    "             label=labels[i])\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('error values',fontsize=14)\n",
    "plt.title('error value for different penaty level', fontsize=18)\n",
    "fig.get_axes()[0].annotate('compare the performance of ridge and lasso on linearregression',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['linear_ridge', 'linear_lasso', 'log_ridge', 'log_lasso']\n",
    "\n",
    "fig = plt.figure(figsize=(18., 9.))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(2, nr_tests):  \n",
    "    plt.plot(iteration_ids, f_obj[:,i], next(linecycler),  \n",
    "             color=next(colors), label=labels[i])\n",
    "plt.legend(loc=1,fontsize=14)\n",
    "plt.xlabel('number of iterations',fontsize=14)\n",
    "plt.ylabel('obj values', fontsize=14)\n",
    "plt.title('objective value for different penaty level',fontsize=18)\n",
    "#plt.ylim([0, ])\n",
    "\n",
    "### plot the error value for differnt penalty level\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(2, nr_tests):  \n",
    "    plt.plot(iteration_ids, errors[:,i],next(linecycler),color=next(colors),\n",
    "             label=labels[i])\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.xlabel('number of iterations', fontsize=14)\n",
    "plt.ylabel('error values',fontsize=14)\n",
    "plt.title('error value for different penaty level', fontsize=18)\n",
    "fig.get_axes()[0].annotate('compare the performance of ridge and lasso on logistic regression',\n",
    "                           (0.5, 0.945), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 20))\n",
    "for i in range(nr_tests):  \n",
    "    si = s_levels[i]    \n",
    "    ax = fig.add_subplot(nr_tests,1,i+1)\n",
    "    ax.stem(xs[:,i], label=labels[i])    \n",
    "    ax.legend(loc=1, fontsize=14)\n",
    "    ax.set_ylabel('x values',fontsize=14)\n",
    "    \n",
    "fig.get_axes()[0].annotate('x(minimizer)values for different proximator and regression',\n",
    "                           (0.5, 0.94), xycoords='figure fraction', ha='center', \n",
    "                            fontsize=24 )\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms comparison and numerical experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Questions</b>:\n",
    "     <ul>\n",
    "      <li>Compute a precise minimum and a precise minimizer of the linear regression with ridge \n",
    "  penalization problem using the parameters give above. This can be done by using fista with \n",
    "  1000 iterations.</li>\n",
    "    <li>Compare the convergences of ISTA and FISTA, in terms of distance to the minimum and \n",
    "  distance to the minimizer. Do your plots using a logarithmic scale of the y-axis.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>Compare the solution you obtain with ista and fista with the true parameter `coefs` of\n",
    "  the model. This can be done with `plt.stem` plots.</li>\n",
    "    <li>In linear regression and logistic regression, study the influence of the correlation \n",
    "  of the features on the performance of the optimization algorithms. Explain.</li>\n",
    "    <li>In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.</li>\n",
    "    <li>In linear regression and logistic regression, compare the performance of the optimization\n",
    "  algorithms for ridge and lasso penalizations. Explain</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
